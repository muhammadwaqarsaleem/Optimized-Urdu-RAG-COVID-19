{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPBzhdVo9UUGTzPaee/CqYj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muhammadwaqarsaleem/Optimized-Urdu-RAG-COVID-19/blob/main/Urdu_RAG_Final_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AeWXGawru7k6"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Environment Setup & Configuration\n",
        "# ==========================================\n",
        "# This cell installs all required libraries and mounts Google Drive.\n",
        "# It uses %%capture to keep the output clean for presentation.\n",
        "\n",
        "%%capture\n",
        "print(\"üöÄ Initializing the Urdu RAG Environment...\")\n",
        "\n",
        "# 1. Install Libraries\n",
        "# - transformers/bitsandbytes/accelerate: For running the Qwen LLM\n",
        "# - sentence-transformers/faiss-cpu: For the Retrieval system\n",
        "# - ipywidgets: For the interactive demo interface\n",
        "!pip install -q --upgrade transformers bitsandbytes accelerate sentence-transformers faiss-cpu ipywidgets\n",
        "\n",
        "# 2. Mount Google Drive\n",
        "# We need this to load your fine-tuned Dense Retriever and your Corpus file.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 3. Import Libraries\n",
        "import torch\n",
        "import time\n",
        "import json\n",
        "import os\n",
        "import faiss\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# 4. Define File Paths (Config)\n",
        "# These point to the files you created in your training notebook.\n",
        "DENSE_MODEL_PATH = \"/content/drive/MyDrive/models/urdu_dense_retriever_best\"\n",
        "CORPUS_PATH = \"/content/drive/MyDrive/data/urdu_covid_passages_min.jsonl\"\n",
        "GENERATOR_ID = \"Qwen/Qwen2.5-7B-Instruct\"  # We use the Base model as it performed best (85+ BLEU)\n",
        "\n",
        "print(\"‚úÖ Environment Ready! Proceed to Cell 2.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support for third party widgets will remain active for the duration of the session. To disable support:"
      ],
      "metadata": {
        "id": "rM2uyo5dJSim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ],
      "metadata": {
        "id": "_U9nWatux1QA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Load Models, Build Index & Define Logic\n",
        "# ===============================================\n",
        "print(\"‚è≥ Loading RAG System... This may take 2-3 minutes.\")\n",
        "\n",
        "# --- PART A: Load the Retriever ---\n",
        "# We try to load your fine-tuned model. If missing, we fallback to the base model.\n",
        "if os.path.exists(DENSE_MODEL_PATH):\n",
        "    print(f\"   üìÇ Loading Fine-Tuned Retriever from: {DENSE_MODEL_PATH}\")\n",
        "    embedder = SentenceTransformer(DENSE_MODEL_PATH).to(\"cuda\")\n",
        "else:\n",
        "    print(\"   ‚ö†Ô∏è Fine-tuned model not found. Loading Base Retriever (Fallback).\")\n",
        "    embedder = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\").to(\"cuda\")\n",
        "\n",
        "# --- PART B: Load Data & Build Index ---\n",
        "print(\"   üèóÔ∏è Building Search Index (FAISS)...\")\n",
        "corpus_texts = []\n",
        "corpus_ids = []\n",
        "\n",
        "# Check if corpus exists before loading\n",
        "if os.path.exists(CORPUS_PATH):\n",
        "    with open(CORPUS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            item = json.loads(line)\n",
        "            corpus_texts.append(item[\"text\"])\n",
        "            corpus_ids.append(item[\"id\"])\n",
        "\n",
        "    # Create embeddings for the search engine\n",
        "    passage_embeddings = embedder.encode(corpus_texts, convert_to_numpy=True, show_progress_bar=False)\n",
        "\n",
        "    # Initialize FAISS (Vector Database)\n",
        "    faiss.normalize_L2(passage_embeddings)\n",
        "    index = faiss.IndexFlatIP(passage_embeddings.shape[1])\n",
        "    index.add(passage_embeddings)\n",
        "    print(f\"      - Indexed {len(corpus_texts)} documents.\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"‚ùå Critical Error: Corpus file not found at {CORPUS_PATH}\")\n",
        "\n",
        "# --- PART C: Load the Generator (Qwen 2.5) ---\n",
        "print(f\"   üß† Loading Generator: {GENERATOR_ID} (4-bit)...\")\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(GENERATOR_ID)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    GENERATOR_ID, quantization_config=bnb_config, device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# --- PART D: Define the RAG Function ---\n",
        "def ask_ur_rag(query):\n",
        "    \"\"\"\n",
        "    1. Retrieve relevant docs using the Dense Retriever.\n",
        "    2. Format a prompt for the Qwen model.\n",
        "    3. Generate the answer in Urdu.\n",
        "    \"\"\"\n",
        "    # 1. Retrieval\n",
        "    q_emb = embedder.encode([query], convert_to_numpy=True)\n",
        "    faiss.normalize_L2(q_emb)\n",
        "    D, I = index.search(q_emb, 3) # Get Top 3 Documents\n",
        "\n",
        "    # Fetch text for the indices found\n",
        "    docs = [(corpus_ids[i], corpus_texts[i]) for idx, i in enumerate(I[0])]\n",
        "\n",
        "    # Create Context String\n",
        "    context = \"\\n\".join([f\"- {d[1]}\" for d in docs])\n",
        "\n",
        "    # 2. Prompt Engineering\n",
        "    sys_prompt = \"ÿ¢Ÿæ ÿß€å⁄© ŸÖÿß€Åÿ± ⁄àÿß⁄©Ÿπÿ± €Å€å⁄∫€î ŸÜ€å⁄Ü€í ÿØ€å ⁄Øÿ¶€å ŸÖÿπŸÑŸàŸÖÿßÿ™ ⁄©€å ÿ®ŸÜ€åÿßÿØ Ÿæÿ± ÿ≥ŸàÿßŸÑ ⁄©ÿß ÿßÿ±ÿØŸà ŸÖ€å⁄∫ ÿ¨Ÿàÿßÿ® ÿØ€å⁄∫€î\"\n",
        "    user_prompt = f\"ŸÖÿπŸÑŸàŸÖÿßÿ™:\\n{context}\\n\\nÿ≥ŸàÿßŸÑ: {query}\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": sys_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt}\n",
        "    ]\n",
        "\n",
        "    # 3. Generation\n",
        "    text_input = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    model_inputs = tokenizer([text_input], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(\n",
        "            **model_inputs,\n",
        "            max_new_tokens=256,\n",
        "            temperature=0.3, # Low temperature for factual consistency\n",
        "            do_sample=True\n",
        "        )\n",
        "\n",
        "    # Decode and clean response\n",
        "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "    # Extract only the assistant's response (remove the prompt)\n",
        "    if \"assistant\" in response:\n",
        "        response = response.split(\"assistant\")[-1].strip()\n",
        "\n",
        "    return response, docs\n",
        "\n",
        "print(\"‚úÖ System Fully Operational! Ready for Demo.\")"
      ],
      "metadata": {
        "id": "9XNcR12fvEph",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "a63fd29f-d521-4e38-eef4-8f776f536114"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ Loading RAG System... This may take 2-3 minutes.\n",
            "   üìÇ Loading Fine-Tuned Retriever from: /content/drive/MyDrive/models/urdu_dense_retriever_best\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "95c90f126c7842079bae1ca035fbbc6f"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   üèóÔ∏è Building Search Index (FAISS)...\n",
            "      - Indexed 60 documents.\n",
            "   üß† Loading Generator: Qwen/Qwen2.5-7B-Instruct (4-bit)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fd882dd2500c45b2b4a3c49bc2c68f46"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bff255d42972413da08e976ba45aa4df"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          }
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "130ecb4376d1440ea7f1e61bbeb7916a"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb1c8b617f4d4542995371f19c162f17"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5e93fb979d664fa09d16e62a62a4b0ad"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f8b5f6e7d28644f7acf467830fdec996"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "40121d09f592425a9ec550a4dae9d2dc"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0ba7650e7556429e8f8f3dacf7187c75"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/339 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fbd21fefae22495e89caf063a5eb5376"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a82ddd3f1ba4306a8e800d597a6908b"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ System Fully Operational! Ready for Demo.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3 (Backup): Simple Text Loop\n",
        "# ================================\n",
        "print(\"ü©∫ Urdu COVID-19 AI Assistant (Simple Mode)\")\n",
        "print(\"Type 'exit' to stop.\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "while True:\n",
        "    print(\"\\nüëá ŸÜ€å⁄Ü€í ÿ≥ŸàÿßŸÑ ŸÑ⁄©⁄æ€å⁄∫ (Enter Question):\")\n",
        "    query = input() # Standard Python Input\n",
        "\n",
        "    if query.lower() in ['exit', 'quit', 'x']:\n",
        "        print(\"üëã Allah Hafiz!\")\n",
        "        break\n",
        "\n",
        "    if not query.strip(): continue\n",
        "\n",
        "    print(f\"\\nü§î Thinking...\")\n",
        "    try:\n",
        "        start = time.time()\n",
        "        ans, docs = ask_ur_rag(query)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*40)\n",
        "        print(f\"üì¢ ÿ¨Ÿàÿßÿ®: {ans}\")\n",
        "        print(\"=\"*40)\n",
        "        print(f\"üìö ÿ≠ŸàÿßŸÑ€Å: {docs[0][1][:100]}...\")\n",
        "        print(f\"‚è±Ô∏è Time: {time.time()-start:.2f}s\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")"
      ],
      "metadata": {
        "id": "ImvL8ZmAvBcO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d9d526b-9959-4813-adca-c6b94c0b36c5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü©∫ Urdu COVID-19 AI Assistant (Simple Mode)\n",
            "Type 'exit' to stop.\n",
            "----------------------------------------\n",
            "\n",
            "üëá ŸÜ€å⁄Ü€í ÿ≥ŸàÿßŸÑ ŸÑ⁄©⁄æ€å⁄∫ (Enter Question):\n",
            "ÿ≥ŸÖÿßÿ¨€å ŸÅÿßÿµŸÑ€Å ÿ±⁄©⁄æŸÜ€í ⁄©€å ÿß€ÅŸÖ€åÿ™ ⁄©€åÿß €Å€íÿü\n",
            "\n",
            "ü§î Thinking...\n",
            "\n",
            "========================================\n",
            "üì¢ ÿ¨Ÿàÿßÿ®: ÿ≥ŸÖÿßÿ¨€å ŸÅÿßÿµŸÑ€Å ÿ±⁄©⁄æŸÜ€í ⁄©€å ÿß€ÅŸÖ€åÿ™ Ÿà€å⁄©ÿ≥€åŸÜ€åÿ¥ŸÜ ⁄©€å ŸÖ€ÅŸÖÿßÿ™ ŸÖ€å⁄∫ ÿ®€Åÿ™ ÿ≤€åÿßÿØ€Å €Å€í€î ÿßÿ≥€í ⁄©⁄Ü⁄æ ÿØÿ±ÿ¨ ÿ∞€åŸÑ ÿ∑ÿ±€åŸÇ€í ÿ≥€í ÿ™Ÿàÿ∂€åÿ≠ ÿØ€åÿß ÿ¨ÿß ÿ≥⁄©ÿ™ÿß €Å€í:\n",
            "\n",
            "1. ÿ¢⁄Øÿß€Å€å: ÿ≥ŸÖÿßÿ¨€å ŸÅÿßÿµŸÑ€Å ÿ±⁄©⁄æŸÜ€í ÿ≥€í ŸÑŸà⁄Ø ÿßŸæŸÜÿß ÿµÿ≠ÿ™€å ÿ≥€åÿ≥ÿ™ŸÖ ⁄©€í ÿ®ÿßÿ±€í ŸÖ€å⁄∫ ÿß⁄Ü⁄æ€å ÿ∑ÿ±ÿ≠ ÿ¢⁄Øÿß€Å €ÅŸà ÿ¨ÿßÿ™€í €Å€å⁄∫€î Ÿà€å⁄©ÿ≥€åŸÜ€åÿ¥ŸÜ ⁄©€í ŸÅÿßÿ¶ÿØ€í ÿßŸàÿ± ÿÆÿ∑ÿ±€í ⁄©€í ÿ®ÿßÿ±€í ŸÖ€å⁄∫ ŸÖÿπŸÑŸàŸÖÿßÿ™ ⁄©ÿß ÿßŸÜÿ™ÿ¥ÿßÿ± ⁄©ÿ±ŸÜ€í ÿ≥€í ŸÑŸà⁄Ø ÿßŸæŸÜ€í ÿµÿ≠ÿ™€å ÿ≥€åÿ≥ÿ™ŸÖ ⁄©€í ÿ®ÿßÿ±€í ŸÖ€å⁄∫ ÿ®€Åÿ™ÿ± ŸÅŸáŸÖ€å €ÅŸàÿ™€å €Å€í€î\n",
            "\n",
            "2. ÿ±ÿ≥ÿßÿ¶€å: ÿ≥ŸÖÿßÿ¨€å ŸÅÿßÿµŸÑ€Å ÿ±⁄©⁄æŸÜ€í ÿ≥€í ÿß€å⁄© ŸÖŸÑ⁄© ⁄©€í ŸÖÿÆÿ™ŸÑŸÅ ŸÖŸÜÿßÿ∑ŸÇŸà⁄∫ ŸÖ€å⁄∫ ÿ®€åŸÜ\n",
            "========================================\n",
            "üìö ÿ≠ŸàÿßŸÑ€Å: Ÿà€å⁄©ÿ≥€åŸÜ ⁄©€å ÿßŸÅÿßÿØ€åÿ™ ŸàŸÇÿ™ ⁄©€í ÿ≥ÿßÿ™⁄æ ⁄©ŸÖ €ÅŸà ÿ≥⁄©ÿ™€å €Å€íÿå ÿßÿ≥ ŸÑ€å€í ÿ®Ÿàÿ≥Ÿπÿ± ÿÆŸàÿ±ÿß⁄©€å⁄∫ ÿ®ÿπÿ∂ ⁄Øÿ±ŸàŸæÿ≥ ⁄©€í ŸÑ€å€í ÿ™ÿ¨Ÿà€åÿ≤ ⁄©€å ÿ¨ÿßÿ™€å €Å€å⁄∫ÿõ...\n",
            "‚è±Ô∏è Time: 24.03s\n",
            "\n",
            "üëá ŸÜ€å⁄Ü€í ÿ≥ŸàÿßŸÑ ŸÑ⁄©⁄æ€å⁄∫ (Enter Question):\n",
            "⁄©ŸàŸà⁄à-19 Ÿà€å⁄©ÿ≥€åŸÜ ÿ¨ÿ≥ŸÖ ŸÖ€å⁄∫ ⁄©€åÿ≥€í ⁄©ÿßŸÖ ⁄©ÿ±ÿ™€å €Å€íÿü\n",
            "\n",
            "ü§î Thinking...\n",
            "\n",
            "========================================\n",
            "üì¢ ÿ¨Ÿàÿßÿ®: ⁄©ŸàŸà⁄à-19 Ÿà€å⁄©ÿ≥€åŸÜ ÿ¨ÿ≥ŸÖ ŸÖ€å⁄∫ ⁄©ÿßŸÖ ⁄©ÿ±ÿ™€å €Å€í ⁄©€Åÿß⁄∫ ⁄©€Åÿß⁄∫ ⁄©ÿßŸÖ ⁄©ÿ±ÿ™€å €Å€í ŸÑ€å⁄©ŸÜ ÿßÿ≥ ⁄©€å ⁄©ŸÑ€å€Å ⁄©ÿß ÿ™ŸÅÿµ€åŸÑ€å ÿ™Ÿàÿ∂€åÿ≠ ÿØ€åŸÜ€í ⁄©€í ŸÑ€å€í ⁄©ÿßŸÅ€å ŸÖÿπŸÑŸàŸÖÿßÿ™ ŸÜ€Å€å⁄∫ €Å€å⁄∫€î ŸÑ€å⁄©ŸÜ ŸÖ€å⁄∫ ÿ¢Ÿæ ⁄©Ÿà ⁄©⁄Ü⁄æ ÿßÿ≥ÿßÿ≥€å ÿßÿ∑ŸÑÿßÿπÿßÿ™ ÿØ€å⁄∫ ⁄Üÿß€Åÿ™ÿß €ÅŸà⁄∫:\n",
            "\n",
            "1. **ÿ¢ÿ±⁄à€åŸÜÿ≥€å ⁄©€í ÿ∞ÿ±€åÿπ€Å**: Ÿà€å⁄©ÿ≥€åŸÜ ŸÜ€í ÿ¨ÿ≥ŸÖ ⁄©€í ŸÜÿ∏ÿßŸÖ ⁄©Ÿà ÿ¢ÿ±⁄à€åŸÜÿ≥€å (ÿ¢ÿ±⁄à€åŸÜÿ≥€å ⁄©€í ŸÑ€å€í ⁄©€ÅŸÑÿßÿ™ÿß €Å€í) ÿØ€å ⁄Øÿ¶€å €Å€í€î ÿß€å⁄© ÿ¢ÿ±⁄à€åŸÜÿ≥€å ⁄©€å Ÿàÿ¨€Å ÿ≥€í ÿ¨ÿ≥ŸÖ ⁄©ÿß ŸÜÿ∏ÿßŸÖ ⁄©Ÿà ⁄©ŸàŸà⁄à-19 ⁄©€í ŸÖÿ±ÿ∂€åŸà⁄∫ ⁄©€í ŸÖÿ®ÿßÿ±⁄© ⁄©ÿßÿ±⁄©ÿ±ÿØ⁄Ø€åŸà⁄∫ ⁄©€í ÿ≥ÿßÿ™⁄æ ŸÖ€åŸÑŸÜ€å ⁄©€å ÿ∂ÿ±Ÿàÿ±ÿ™ €ÅŸà⁄Ø\n",
            "========================================\n",
            "üìö ÿ≠ŸàÿßŸÑ€Å: ⁄©ŸàŸà⁄à-19 ⁄©€í ÿØŸàÿ±ÿßŸÜ ÿ≥ŸÅÿ± ÿ≥€í Ÿæ€ÅŸÑ€í Ÿπ€åÿ≥ŸπŸÜ⁄Ø ÿßŸàÿ± ŸÇÿ±ŸÜÿ∑€åŸÜ€Å ⁄©€í ŸÇŸàÿßÿπÿØ Ÿæÿ± ÿπŸÖŸÑ ⁄©ÿ±ŸÜÿß ÿ≥ŸÅÿ±€å ÿÆÿ∑ÿ±ÿßÿ™ ⁄©ŸÖ ⁄©ÿ±ÿ™ÿß €Å€íÿõ ŸÖŸÇÿßŸÖ€å ŸÇŸà...\n",
            "‚è±Ô∏è Time: 22.74s\n",
            "\n",
            "üëá ŸÜ€å⁄Ü€í ÿ≥ŸàÿßŸÑ ŸÑ⁄©⁄æ€å⁄∫ (Enter Question):\n",
            "exit\n",
            "üëã Allah Hafiz!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.disable_custom_widget_manager()"
      ],
      "metadata": {
        "id": "h63djNmNJSim"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support for third party widgets will remain active for the duration of the session. To disable support:"
      ],
      "metadata": {
        "id": "IP9d2KSh2MBS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This brings us to the end of our research. Overall, it sets ground for more extensive future research into this domain. We are making a comprehensive README.md file for this whole project to facilitate the use of our work for future research."
      ],
      "metadata": {
        "id": "2C7KtZ49uJ2t"
      }
    }
  ]
}