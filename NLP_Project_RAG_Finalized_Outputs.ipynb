{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmedsaalman/low-resource-rag-comparison/blob/main/NLP_Project_RAG_Finalized_Outputs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "lzOG_Jcoakae",
        "outputId": "9a015385-8423-4a16-e496-e3ba1f9f848b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Cell 1: Install required libraries (run this cell first and one by one all required libraries will be installed)\n",
        "# - transformers: model + generation\n",
        "# - sentence-transformers: dense embeddings / fine-tuning helpers\n",
        "# - faiss-cpu (or faiss-gpu if GPU available)\n",
        "# - rank_bm25: BM25 baseline\n",
        "# - datasets: convenient JSONL loading\n",
        "# - evaluate / sacrebleu: BLEU/chrF metrics\n",
        "# - tqdm: progress bars\n",
        "# - accelerate (optional) for distributed/faster training\n",
        "!pip install -q transformers sentence-transformers faiss-cpu rank_bm25 datasets evaluate sacrebleu tqdm accelerate\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time\n",
        "\n",
        "# Force Colab to sync the notebook state\n",
        "os.system(\"echo 'Saving notebook state...'\")\n",
        "time.sleep(2)\n",
        "\n",
        "print(\"✅ Notebook state synced. Now use File → Save a copy in GitHub or Download .ipynb\")\n"
      ],
      "metadata": {
        "id": "qZsW5TH-y_TJ",
        "outputId": "62be104e-a036-409c-d108-e8fcf4a1a668",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Notebook state synced. Now use File → Save a copy in GitHub or Download .ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to google drive if not already connected\n",
        "# 2. Mount Google Drive\n",
        "# We need this to load your fine-tuned Dense Retriever and your Corpus file.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFnrxVRazzm1",
        "outputId": "3ae2f3cd-830b-44e7-fbac-21488825d728"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RbSs_eoZPLJV",
        "outputId": "7ecdfd48-2508-44f1-a0b7-24c209538c42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.13.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.6)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.4.1)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.12/dist-packages (2.5.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (22.0.0)\n",
            "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (3.2.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2025.11.3)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.12.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement faiss-gpu (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for faiss-gpu\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install faiss-cpu --no-cache-dir\n",
        "!pip install evaluate datasets sacrebleu\n",
        "!pip install faiss-gpu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rM5UqSd5I4q1"
      },
      "outputs": [],
      "source": [
        "# Optional cell\n",
        "# To add all the required files run\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "H7rCqtdUcttO",
        "outputId": "04cf580a-3438-4b2a-d814-234235d30e97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Package                                  Version\n",
            "---------------------------------------- --------------------\n",
            "absl-py                                  1.4.0\n",
            "accelerate                               1.12.0\n",
            "access                                   1.1.9\n",
            "affine                                   2.4.0\n",
            "aiofiles                                 24.1.0\n",
            "aiohappyeyeballs                         2.6.1\n",
            "aiohttp                                  3.13.2\n",
            "aiosignal                                1.4.0\n",
            "aiosqlite                                0.21.0\n",
            "alabaster                                1.0.0\n",
            "albucore                                 0.0.24\n",
            "albumentations                           2.0.8\n",
            "ale-py                                   0.11.2\n",
            "alembic                                  1.17.2\n",
            "altair                                   5.5.0\n",
            "annotated-types                          0.7.0\n",
            "antlr4-python3-runtime                   4.9.3\n",
            "anyio                                    4.11.0\n",
            "anywidget                                0.9.21\n",
            "argon2-cffi                              25.1.0\n",
            "argon2-cffi-bindings                     25.1.0\n",
            "array_record                             0.8.3\n",
            "arrow                                    1.4.0\n",
            "arviz                                    0.22.0\n",
            "astropy                                  7.1.1\n",
            "astropy-iers-data                        0.2025.11.24.0.39.11\n",
            "astunparse                               1.6.3\n",
            "atpublic                                 5.1\n",
            "attrs                                    25.4.0\n",
            "audioread                                3.1.0\n",
            "Authlib                                  1.6.5\n",
            "autograd                                 1.8.0\n",
            "babel                                    2.17.0\n",
            "backcall                                 0.2.0\n",
            "beartype                                 0.22.6\n",
            "beautifulsoup4                           4.13.5\n",
            "betterproto                              2.0.0b6\n",
            "bigframes                                2.29.1\n",
            "bigquery-magics                          0.10.3\n",
            "bleach                                   6.3.0\n",
            "blinker                                  1.9.0\n",
            "blis                                     1.3.3\n",
            "blobfile                                 3.1.0\n",
            "blosc2                                   3.11.1\n",
            "bokeh                                    3.7.3\n",
            "Bottleneck                               1.4.2\n",
            "bqplot                                   0.12.45\n",
            "branca                                   0.8.2\n",
            "brotli                                   1.2.0\n",
            "CacheControl                             0.14.4\n",
            "cachetools                               6.2.2\n",
            "catalogue                                2.0.10\n",
            "certifi                                  2025.11.12\n",
            "cffi                                     2.0.0\n",
            "chardet                                  5.2.0\n",
            "charset-normalizer                       3.4.4\n",
            "chex                                     0.1.90\n",
            "clarabel                                 0.11.1\n",
            "click                                    8.3.1\n",
            "click-plugins                            1.1.1.2\n",
            "cligj                                    0.7.2\n",
            "cloudpathlib                             0.23.0\n",
            "cloudpickle                              3.1.2\n",
            "cmake                                    3.31.10\n",
            "cmdstanpy                                1.3.0\n",
            "colorama                                 0.4.6\n",
            "colorcet                                 3.1.0\n",
            "colorlover                               0.3.0\n",
            "colour                                   0.1.5\n",
            "community                                1.0.0b1\n",
            "confection                               0.1.5\n",
            "cons                                     0.4.7\n",
            "contourpy                                1.3.3\n",
            "cramjam                                  2.11.0\n",
            "cryptography                             43.0.3\n",
            "cuda-bindings                            12.9.4\n",
            "cuda-core                                0.3.2\n",
            "cuda-pathfinder                          1.3.2\n",
            "cuda-python                              12.9.4\n",
            "cuda-toolkit                             12.9.1\n",
            "cudf-cu12                                25.10.0\n",
            "cudf-polars-cu12                         25.10.0\n",
            "cufflinks                                0.17.3\n",
            "cuml-cu12                                25.10.0\n",
            "cupy-cuda12x                             13.6.0\n",
            "curl_cffi                                0.13.0\n",
            "cvxopt                                   1.3.2\n",
            "cvxpy                                    1.6.7\n",
            "cycler                                   0.12.1\n",
            "cyipopt                                  1.5.0\n",
            "cymem                                    2.0.13\n",
            "Cython                                   3.0.12\n",
            "dask                                     2025.9.1\n",
            "dask-cuda                                25.10.0\n",
            "dask-cudf-cu12                           25.10.0\n",
            "dataproc-spark-connect                   0.8.3\n",
            "datasets                                 4.0.0\n",
            "db-dtypes                                1.4.4\n",
            "dbus-python                              1.2.18\n",
            "debugpy                                  1.8.15\n",
            "decorator                                4.4.2\n",
            "defusedxml                               0.7.1\n",
            "deprecation                              2.1.0\n",
            "diffusers                                0.35.2\n",
            "dill                                     0.3.8\n",
            "distributed                              2025.9.1\n",
            "distributed-ucxx-cu12                    0.46.0\n",
            "distro                                   1.9.0\n",
            "dlib                                     19.24.6\n",
            "dm-tree                                  0.1.9\n",
            "docstring_parser                         0.17.0\n",
            "docutils                                 0.21.2\n",
            "dopamine_rl                              4.1.2\n",
            "duckdb                                   1.3.2\n",
            "earthengine-api                          1.5.24\n",
            "easydict                                 1.13\n",
            "editdistance                             0.8.1\n",
            "eerepr                                   0.1.2\n",
            "einops                                   0.8.1\n",
            "en_core_web_sm                           3.8.0\n",
            "entrypoints                              0.4\n",
            "esda                                     2.8.0\n",
            "et_xmlfile                               2.0.0\n",
            "etils                                    1.13.0\n",
            "etuples                                  0.3.10\n",
            "evaluate                                 0.4.6\n",
            "faiss-cpu                                1.13.1\n",
            "Farama-Notifications                     0.0.4\n",
            "fastai                                   2.8.5\n",
            "fastapi                                  0.118.3\n",
            "fastcore                                 1.8.16\n",
            "fastdownload                             0.0.7\n",
            "fastjsonschema                           2.21.2\n",
            "fastprogress                             1.0.3\n",
            "fastrlock                                0.8.3\n",
            "fasttransform                            0.0.2\n",
            "ffmpy                                    1.0.0\n",
            "filelock                                 3.20.0\n",
            "fiona                                    1.10.1\n",
            "firebase-admin                           6.9.0\n",
            "Flask                                    3.1.2\n",
            "flatbuffers                              25.9.23\n",
            "flax                                     0.10.7\n",
            "folium                                   0.20.0\n",
            "fonttools                                4.60.1\n",
            "fqdn                                     1.5.1\n",
            "frozendict                               2.4.7\n",
            "frozenlist                               1.8.0\n",
            "fsspec                                   2025.3.0\n",
            "future                                   1.0.0\n",
            "gast                                     0.6.0\n",
            "gcsfs                                    2025.3.0\n",
            "GDAL                                     3.8.4\n",
            "gdown                                    5.2.0\n",
            "geemap                                   0.35.3\n",
            "geocoder                                 1.38.1\n",
            "geographiclib                            2.1\n",
            "geopandas                                1.1.1\n",
            "geopy                                    2.4.1\n",
            "giddy                                    2.3.6\n",
            "gin-config                               0.5.0\n",
            "gitdb                                    4.0.12\n",
            "GitPython                                3.1.45\n",
            "glob2                                    0.7\n",
            "google                                   3.0.0\n",
            "google-adk                               1.19.0\n",
            "google-ai-generativelanguage             0.6.15\n",
            "google-api-core                          2.28.1\n",
            "google-api-python-client                 2.187.0\n",
            "google-auth                              2.43.0\n",
            "google-auth-httplib2                     0.2.1\n",
            "google-auth-oauthlib                     1.2.2\n",
            "google-cloud-aiplatform                  1.128.0\n",
            "google-cloud-appengine-logging           1.7.0\n",
            "google-cloud-audit-log                   0.4.0\n",
            "google-cloud-bigquery                    3.38.0\n",
            "google-cloud-bigquery-connection         1.19.0\n",
            "google-cloud-bigquery-storage            2.34.0\n",
            "google-cloud-bigtable                    2.34.0\n",
            "google-cloud-core                        2.5.0\n",
            "google-cloud-dataproc                    5.23.0\n",
            "google-cloud-datastore                   2.21.0\n",
            "google-cloud-discoveryengine             0.13.12\n",
            "google-cloud-firestore                   2.21.0\n",
            "google-cloud-functions                   1.21.0\n",
            "google-cloud-language                    2.18.0\n",
            "google-cloud-logging                     3.12.1\n",
            "google-cloud-monitoring                  2.28.0\n",
            "google-cloud-resource-manager            1.15.0\n",
            "google-cloud-secret-manager              2.25.0\n",
            "google-cloud-spanner                     3.59.0\n",
            "google-cloud-speech                      2.34.0\n",
            "google-cloud-storage                     3.6.0\n",
            "google-cloud-trace                       1.17.0\n",
            "google-cloud-translate                   3.23.0\n",
            "google-colab                             1.0.0\n",
            "google-crc32c                            1.7.1\n",
            "google-genai                             1.52.0\n",
            "google-generativeai                      0.8.5\n",
            "google-pasta                             0.2.0\n",
            "google-resumable-media                   2.8.0\n",
            "googleapis-common-protos                 1.72.0\n",
            "googledrivedownloader                    1.1.0\n",
            "gradio                                   5.50.0\n",
            "gradio_client                            1.14.0\n",
            "graphviz                                 0.21\n",
            "greenlet                                 3.2.4\n",
            "groovy                                   0.1.2\n",
            "grpc-google-iam-v1                       0.14.3\n",
            "grpc-interceptor                         0.15.4\n",
            "grpcio                                   1.76.0\n",
            "grpcio-status                            1.71.2\n",
            "grpclib                                  0.4.8\n",
            "gspread                                  6.2.1\n",
            "gspread-dataframe                        4.0.0\n",
            "gym                                      0.25.2\n",
            "gym-notices                              0.1.0\n",
            "gymnasium                                1.2.2\n",
            "h11                                      0.16.0\n",
            "h2                                       4.3.0\n",
            "h5netcdf                                 1.7.3\n",
            "h5py                                     3.15.1\n",
            "hdbscan                                  0.8.40\n",
            "hf_transfer                              0.1.9\n",
            "hf-xet                                   1.2.0\n",
            "highspy                                  1.12.0\n",
            "holidays                                 0.85\n",
            "holoviews                                1.22.0\n",
            "hpack                                    4.1.0\n",
            "html5lib                                 1.1\n",
            "httpcore                                 1.0.9\n",
            "httpimport                               1.4.1\n",
            "httplib2                                 0.31.0\n",
            "httpx                                    0.28.1\n",
            "httpx-sse                                0.4.3\n",
            "huggingface-hub                          0.36.0\n",
            "humanize                                 4.14.0\n",
            "hyperframe                               6.1.0\n",
            "hyperopt                                 0.2.7\n",
            "ibis-framework                           9.5.0\n",
            "idna                                     3.11\n",
            "ImageIO                                  2.37.2\n",
            "imageio-ffmpeg                           0.6.0\n",
            "imagesize                                1.4.1\n",
            "imbalanced-learn                         0.14.0\n",
            "immutabledict                            4.2.2\n",
            "importlib_metadata                       8.7.0\n",
            "importlib_resources                      6.5.2\n",
            "imutils                                  0.5.4\n",
            "inequality                               1.1.2\n",
            "inflect                                  7.5.0\n",
            "iniconfig                                2.3.0\n",
            "intel-cmplr-lib-ur                       2025.3.1\n",
            "intel-openmp                             2025.3.1\n",
            "ipyevents                                2.0.4\n",
            "ipyfilechooser                           0.6.0\n",
            "ipykernel                                6.17.1\n",
            "ipyleaflet                               0.20.0\n",
            "ipyparallel                              8.8.0\n",
            "ipython                                  7.34.0\n",
            "ipython-genutils                         0.2.0\n",
            "ipython-sql                              0.5.0\n",
            "ipytree                                  0.2.2\n",
            "ipywidgets                               7.7.1\n",
            "isoduration                              20.11.0\n",
            "itsdangerous                             2.2.0\n",
            "jaraco.classes                           3.4.0\n",
            "jaraco.context                           6.0.1\n",
            "jaraco.functools                         4.3.0\n",
            "jax                                      0.7.2\n",
            "jax-cuda12-pjrt                          0.7.2\n",
            "jax-cuda12-plugin                        0.7.2\n",
            "jaxlib                                   0.7.2\n",
            "jeepney                                  0.9.0\n",
            "jieba                                    0.42.1\n",
            "Jinja2                                   3.1.6\n",
            "jiter                                    0.12.0\n",
            "joblib                                   1.5.2\n",
            "jsonpatch                                1.33\n",
            "jsonpickle                               4.1.1\n",
            "jsonpointer                              3.0.0\n",
            "jsonschema                               4.25.1\n",
            "jsonschema-specifications                2025.9.1\n",
            "jupyter_client                           7.4.9\n",
            "jupyter-console                          6.6.3\n",
            "jupyter_core                             5.9.1\n",
            "jupyter-events                           0.12.0\n",
            "jupyter_kernel_gateway                   2.5.2\n",
            "jupyter-leaflet                          0.20.0\n",
            "jupyter_server                           2.14.0\n",
            "jupyter_server_terminals                 0.5.3\n",
            "jupyterlab_pygments                      0.3.0\n",
            "jupyterlab_widgets                       3.0.16\n",
            "jupytext                                 1.18.1\n",
            "kaggle                                   1.7.4.5\n",
            "kagglehub                                0.3.13\n",
            "keras                                    3.10.0\n",
            "keras-hub                                0.21.1\n",
            "keras-nlp                                0.21.1\n",
            "keyring                                  25.7.0\n",
            "keyrings.google-artifactregistry-auth    1.1.2\n",
            "kiwisolver                               1.4.9\n",
            "langchain                                1.1.0\n",
            "langchain-core                           1.1.0\n",
            "langgraph                                1.0.3\n",
            "langgraph-checkpoint                     3.0.1\n",
            "langgraph-prebuilt                       1.0.5\n",
            "langgraph-sdk                            0.2.10\n",
            "langsmith                                0.4.47\n",
            "lark                                     1.3.1\n",
            "launchpadlib                             1.10.16\n",
            "lazr.restfulclient                       0.14.4\n",
            "lazr.uri                                 1.0.6\n",
            "lazy_loader                              0.4\n",
            "libclang                                 18.1.1\n",
            "libcudf-cu12                             25.10.0\n",
            "libcugraph-cu12                          25.10.1\n",
            "libcuml-cu12                             25.10.0\n",
            "libkvikio-cu12                           25.10.0\n",
            "libpysal                                 4.13.0\n",
            "libraft-cu12                             25.10.0\n",
            "librmm-cu12                              25.10.0\n",
            "librosa                                  0.11.0\n",
            "libucx-cu12                              1.19.0\n",
            "libucxx-cu12                             0.46.0\n",
            "lightgbm                                 4.6.0\n",
            "linkify-it-py                            2.0.3\n",
            "llvmlite                                 0.43.0\n",
            "locket                                   1.0.0\n",
            "logical-unification                      0.4.7\n",
            "lxml                                     6.0.2\n",
            "Mako                                     1.3.10\n",
            "mapclassify                              2.10.0\n",
            "Markdown                                 3.10\n",
            "markdown-it-py                           4.0.0\n",
            "MarkupSafe                               3.0.3\n",
            "matplotlib                               3.10.0\n",
            "matplotlib-inline                        0.2.1\n",
            "matplotlib-venn                          1.1.2\n",
            "mcp                                      1.22.0\n",
            "mdit-py-plugins                          0.5.0\n",
            "mdurl                                    0.1.2\n",
            "mgwr                                     2.2.1\n",
            "miniKanren                               1.0.5\n",
            "missingno                                0.5.2\n",
            "mistune                                  3.1.4\n",
            "mizani                                   0.13.5\n",
            "mkl                                      2025.3.0\n",
            "ml_dtypes                                0.5.4\n",
            "mlxtend                                  0.23.4\n",
            "momepy                                   0.10.0\n",
            "more-itertools                           10.8.0\n",
            "moviepy                                  1.0.3\n",
            "mpmath                                   1.3.0\n",
            "msgpack                                  1.1.2\n",
            "multidict                                6.7.0\n",
            "multipledispatch                         1.0.0\n",
            "multiprocess                             0.70.16\n",
            "multitasking                             0.0.12\n",
            "murmurhash                               1.0.15\n",
            "music21                                  9.9.1\n",
            "namex                                    0.1.0\n",
            "narwhals                                 2.12.0\n",
            "natsort                                  8.4.0\n",
            "nbclassic                                1.3.3\n",
            "nbclient                                 0.10.2\n",
            "nbconvert                                7.16.6\n",
            "nbformat                                 5.10.4\n",
            "ndindex                                  1.10.1\n",
            "nest-asyncio                             1.6.0\n",
            "networkx                                 3.6\n",
            "nibabel                                  5.3.2\n",
            "nltk                                     3.9.1\n",
            "notebook                                 6.5.7\n",
            "notebook_shim                            0.2.4\n",
            "numba                                    0.60.0\n",
            "numba-cuda                               0.19.1\n",
            "numexpr                                  2.14.1\n",
            "numpy                                    2.0.2\n",
            "nvidia-cublas-cu12                       12.6.4.1\n",
            "nvidia-cuda-cccl-cu12                    12.9.27\n",
            "nvidia-cuda-cupti-cu12                   12.6.80\n",
            "nvidia-cuda-nvcc-cu12                    12.5.82\n",
            "nvidia-cuda-nvrtc-cu12                   12.6.77\n",
            "nvidia-cuda-runtime-cu12                 12.6.77\n",
            "nvidia-cudnn-cu12                        9.10.2.21\n",
            "nvidia-cufft-cu12                        11.3.0.4\n",
            "nvidia-cufile-cu12                       1.11.1.6\n",
            "nvidia-curand-cu12                       10.3.7.77\n",
            "nvidia-cusolver-cu12                     11.7.1.2\n",
            "nvidia-cusparse-cu12                     12.5.4.2\n",
            "nvidia-cusparselt-cu12                   0.7.1\n",
            "nvidia-ml-py                             13.580.82\n",
            "nvidia-nccl-cu12                         2.27.5\n",
            "nvidia-nvjitlink-cu12                    12.6.85\n",
            "nvidia-nvshmem-cu12                      3.3.20\n",
            "nvidia-nvtx-cu12                         12.6.77\n",
            "nvtx                                     0.2.13\n",
            "nx-cugraph-cu12                          25.10.0\n",
            "oauth2client                             4.1.3\n",
            "oauthlib                                 3.3.1\n",
            "omegaconf                                2.3.0\n",
            "onemkl-license                           2025.3.0\n",
            "openai                                   2.8.1\n",
            "opencv-contrib-python                    4.12.0.88\n",
            "opencv-python                            4.12.0.88\n",
            "opencv-python-headless                   4.12.0.88\n",
            "openpyxl                                 3.1.5\n",
            "opentelemetry-api                        1.37.0\n",
            "opentelemetry-exporter-gcp-logging       1.11.0a0\n",
            "opentelemetry-exporter-gcp-monitoring    1.11.0a0\n",
            "opentelemetry-exporter-gcp-trace         1.11.0\n",
            "opentelemetry-exporter-otlp-proto-common 1.37.0\n",
            "opentelemetry-exporter-otlp-proto-http   1.37.0\n",
            "opentelemetry-proto                      1.37.0\n",
            "opentelemetry-resourcedetector-gcp       1.11.0a0\n",
            "opentelemetry-sdk                        1.37.0\n",
            "opentelemetry-semantic-conventions       0.58b0\n",
            "opt_einsum                               3.4.0\n",
            "optax                                    0.2.6\n",
            "optree                                   0.18.0\n",
            "orbax-checkpoint                         0.11.28\n",
            "orjson                                   3.11.4\n",
            "ormsgpack                                1.12.0\n",
            "osqp                                     1.0.5\n",
            "overrides                                7.7.0\n",
            "packaging                                25.0\n",
            "pandas                                   2.2.2\n",
            "pandas-datareader                        0.10.0\n",
            "pandas-gbq                               0.30.0\n",
            "pandas-stubs                             2.2.2.240909\n",
            "pandocfilters                            1.5.1\n",
            "panel                                    1.8.3\n",
            "param                                    2.3.0\n",
            "parso                                    0.8.5\n",
            "parsy                                    2.2\n",
            "partd                                    1.4.2\n",
            "patsy                                    1.0.2\n",
            "peewee                                   3.18.3\n",
            "peft                                     0.18.0\n",
            "pexpect                                  4.9.0\n",
            "pickleshare                              0.7.5\n",
            "pillow                                   11.3.0\n",
            "pip                                      24.1.2\n",
            "platformdirs                             4.5.0\n",
            "plotly                                   5.24.1\n",
            "plotnine                                 0.14.5\n",
            "pluggy                                   1.6.0\n",
            "plum-dispatch                            2.6.0\n",
            "ply                                      3.11\n",
            "pointpats                                2.5.2\n",
            "polars                                   1.31.0\n",
            "pooch                                    1.8.2\n",
            "portalocker                              3.2.0\n",
            "portpicker                               1.5.2\n",
            "preshed                                  3.0.12\n",
            "prettytable                              3.17.0\n",
            "proglog                                  0.1.12\n",
            "progressbar2                             4.5.0\n",
            "prometheus_client                        0.23.1\n",
            "promise                                  2.3\n",
            "prompt_toolkit                           3.0.52\n",
            "propcache                                0.4.1\n",
            "prophet                                  1.2.1\n",
            "proto-plus                               1.26.1\n",
            "protobuf                                 5.29.5\n",
            "psutil                                   5.9.5\n",
            "psycopg2                                 2.9.11\n",
            "psygnal                                  0.15.0\n",
            "ptyprocess                               0.7.0\n",
            "PuLP                                     3.3.0\n",
            "py-cpuinfo                               9.0.0\n",
            "py4j                                     0.10.9.7\n",
            "pyarrow                                  18.1.0\n",
            "pyasn1                                   0.6.1\n",
            "pyasn1_modules                           0.4.2\n",
            "pycairo                                  1.29.0\n",
            "pycocotools                              2.0.10\n",
            "pycparser                                2.23\n",
            "pycryptodomex                            3.23.0\n",
            "pydantic                                 2.12.3\n",
            "pydantic_core                            2.41.4\n",
            "pydantic-settings                        2.12.0\n",
            "pydata-google-auth                       1.9.1\n",
            "pydot                                    4.0.1\n",
            "pydotplus                                2.0.2\n",
            "PyDrive2                                 1.21.3\n",
            "pydub                                    0.25.1\n",
            "pyerfa                                   2.0.1.5\n",
            "pygame                                   2.6.1\n",
            "pygit2                                   1.19.0\n",
            "Pygments                                 2.19.2\n",
            "PyGObject                                3.48.2\n",
            "PyJWT                                    2.10.1\n",
            "pylibcudf-cu12                           25.10.0\n",
            "pylibcugraph-cu12                        25.10.1\n",
            "pylibraft-cu12                           25.10.0\n",
            "pymc                                     5.26.1\n",
            "pynndescent                              0.5.13\n",
            "pyogrio                                  0.11.1\n",
            "pyomo                                    6.9.5\n",
            "PyOpenGL                                 3.1.10\n",
            "pyOpenSSL                                24.2.1\n",
            "pyparsing                                3.2.5\n",
            "pyperclip                                1.11.0\n",
            "pyproj                                   3.7.2\n",
            "pysal                                    25.7\n",
            "pyshp                                    3.0.2.post1\n",
            "PySocks                                  1.7.1\n",
            "pyspark                                  3.5.1\n",
            "pytensor                                 2.35.1\n",
            "pytest                                   8.4.2\n",
            "python-apt                               0.0.0\n",
            "python-box                               7.3.2\n",
            "python-dateutil                          2.9.0.post0\n",
            "python-dotenv                            1.2.1\n",
            "python-json-logger                       4.0.0\n",
            "python-louvain                           0.16\n",
            "python-multipart                         0.0.20\n",
            "python-slugify                           8.0.4\n",
            "python-snappy                            0.7.3\n",
            "python-utils                             3.9.1\n",
            "pytz                                     2025.2\n",
            "pyviz_comms                              3.0.6\n",
            "PyWavelets                               1.9.0\n",
            "PyYAML                                   6.0.3\n",
            "pyzmq                                    26.2.1\n",
            "quantecon                                0.10.1\n",
            "raft-dask-cu12                           25.10.0\n",
            "rank-bm25                                0.2.2\n",
            "rapids-dask-dependency                   25.10.0\n",
            "rapids-logger                            0.1.19\n",
            "rasterio                                 1.4.3\n",
            "rasterstats                              0.20.0\n",
            "ratelim                                  0.1.6\n",
            "referencing                              0.37.0\n",
            "regex                                    2025.11.3\n",
            "requests                                 2.32.4\n",
            "requests-oauthlib                        2.0.0\n",
            "requests-toolbelt                        1.0.0\n",
            "requirements-parser                      0.9.0\n",
            "rfc3339-validator                        0.1.4\n",
            "rfc3986-validator                        0.1.1\n",
            "rfc3987-syntax                           1.1.0\n",
            "rich                                     13.9.4\n",
            "rmm-cu12                                 25.10.0\n",
            "roman-numerals-py                        3.1.0\n",
            "rpds-py                                  0.29.0\n",
            "rpy2                                     3.5.17\n",
            "rsa                                      4.9.1\n",
            "rtree                                    1.4.1\n",
            "ruff                                     0.14.6\n",
            "sacrebleu                                2.5.1\n",
            "safehttpx                                0.1.7\n",
            "safetensors                              0.7.0\n",
            "scikit-image                             0.25.2\n",
            "scikit-learn                             1.6.1\n",
            "scipy                                    1.16.3\n",
            "scooby                                   0.11.0\n",
            "scs                                      3.2.9\n",
            "seaborn                                  0.13.2\n",
            "SecretStorage                            3.5.0\n",
            "segregation                              2.5.3\n",
            "semantic-version                         2.10.0\n",
            "Send2Trash                               1.8.3\n",
            "sentence-transformers                    5.1.2\n",
            "sentencepiece                            0.2.1\n",
            "sentry-sdk                               2.46.0\n",
            "setuptools                               75.2.0\n",
            "shap                                     0.50.0\n",
            "shapely                                  2.1.2\n",
            "shellingham                              1.5.4\n",
            "simple-parsing                           0.1.7\n",
            "simplejson                               3.20.2\n",
            "simsimd                                  6.5.3\n",
            "six                                      1.17.0\n",
            "sklearn-pandas                           2.2.0\n",
            "slicer                                   0.0.8\n",
            "smart_open                               7.5.0\n",
            "smmap                                    5.0.2\n",
            "sniffio                                  1.3.1\n",
            "snowballstemmer                          3.0.1\n",
            "sortedcontainers                         2.4.0\n",
            "soundfile                                0.13.1\n",
            "soupsieve                                2.8\n",
            "soxr                                     1.0.0\n",
            "spacy                                    3.8.11\n",
            "spacy-legacy                             3.0.12\n",
            "spacy-loggers                            1.0.5\n",
            "spaghetti                                1.7.6\n",
            "spanner-graph-notebook                   1.1.8\n",
            "spglm                                    1.1.0\n",
            "Sphinx                                   8.2.3\n",
            "sphinxcontrib-applehelp                  2.0.0\n",
            "sphinxcontrib-devhelp                    2.0.0\n",
            "sphinxcontrib-htmlhelp                   2.1.0\n",
            "sphinxcontrib-jsmath                     1.0.1\n",
            "sphinxcontrib-qthelp                     2.0.0\n",
            "sphinxcontrib-serializinghtml            2.0.0\n",
            "spint                                    1.0.7\n",
            "splot                                    1.1.7\n",
            "spopt                                    0.7.0\n",
            "spreg                                    1.8.4\n",
            "SQLAlchemy                               2.0.44\n",
            "sqlalchemy-spanner                       1.17.1\n",
            "sqlglot                                  25.20.2\n",
            "sqlparse                                 0.5.3\n",
            "srsly                                    2.5.2\n",
            "sse-starlette                            3.0.3\n",
            "stanio                                   0.5.1\n",
            "starlette                                0.48.0\n",
            "statsmodels                              0.14.5\n",
            "stringzilla                              4.2.3\n",
            "stumpy                                   1.13.0\n",
            "sympy                                    1.14.0\n",
            "tables                                   3.10.2\n",
            "tabulate                                 0.9.0\n",
            "tbb                                      2022.3.0\n",
            "tblib                                    3.2.2\n",
            "tcmlib                                   1.4.1\n",
            "tenacity                                 9.1.2\n",
            "tensorboard                              2.19.0\n",
            "tensorboard-data-server                  0.7.2\n",
            "tensorflow                               2.19.0\n",
            "tensorflow-datasets                      4.9.9\n",
            "tensorflow_decision_forests              1.12.0\n",
            "tensorflow-hub                           0.16.1\n",
            "tensorflow-metadata                      1.17.2\n",
            "tensorflow-probability                   0.25.0\n",
            "tensorflow-text                          2.19.0\n",
            "tensorstore                              0.1.79\n",
            "termcolor                                3.2.0\n",
            "terminado                                0.18.1\n",
            "text-unidecode                           1.3\n",
            "textblob                                 0.19.0\n",
            "tf_keras                                 2.19.0\n",
            "tf-slim                                  1.1.0\n",
            "thinc                                    8.3.10\n",
            "threadpoolctl                            3.6.0\n",
            "tifffile                                 2025.10.16\n",
            "tiktoken                                 0.12.0\n",
            "timm                                     1.0.22\n",
            "tinycss2                                 1.4.0\n",
            "tobler                                   0.12.1\n",
            "tokenizers                               0.22.1\n",
            "toml                                     0.10.2\n",
            "tomlkit                                  0.13.3\n",
            "toolz                                    0.12.1\n",
            "torch                                    2.9.0+cu126\n",
            "torchao                                  0.10.0\n",
            "torchaudio                               2.9.0+cu126\n",
            "torchdata                                0.11.0\n",
            "torchsummary                             1.5.1\n",
            "torchtune                                0.6.1\n",
            "torchvision                              0.24.0+cu126\n",
            "tornado                                  6.5.1\n",
            "tqdm                                     4.67.1\n",
            "traitlets                                5.7.1\n",
            "traittypes                               0.2.3\n",
            "transformers                             4.57.2\n",
            "treelite                                 4.4.1\n",
            "treescope                                0.1.10\n",
            "triton                                   3.5.0\n",
            "tsfresh                                  0.21.1\n",
            "tweepy                                   4.16.0\n",
            "typeguard                                4.4.4\n",
            "typer                                    0.20.0\n",
            "typer-slim                               0.20.0\n",
            "types-pytz                               2025.2.0.20251108\n",
            "types-setuptools                         80.9.0.20250822\n",
            "typing_extensions                        4.15.0\n",
            "typing-inspection                        0.4.2\n",
            "tzdata                                   2025.2\n",
            "tzlocal                                  5.3.1\n",
            "uc-micro-py                              1.0.3\n",
            "ucxx-cu12                                0.46.0\n",
            "umap-learn                               0.5.9.post2\n",
            "umf                                      1.0.2\n",
            "uri-template                             1.3.0\n",
            "uritemplate                              4.2.0\n",
            "urllib3                                  2.5.0\n",
            "uvicorn                                  0.38.0\n",
            "vega-datasets                            0.9.0\n",
            "wadllib                                  1.3.6\n",
            "wandb                                    0.23.0\n",
            "wasabi                                   1.1.3\n",
            "watchdog                                 6.0.0\n",
            "wcwidth                                  0.2.14\n",
            "weasel                                   0.4.3\n",
            "webcolors                                25.10.0\n",
            "webencodings                             0.5.1\n",
            "websocket-client                         1.9.0\n",
            "websockets                               15.0.1\n",
            "Werkzeug                                 3.1.3\n",
            "wheel                                    0.45.1\n",
            "widgetsnbextension                       3.6.10\n",
            "wordcloud                                1.9.4\n",
            "wrapt                                    2.0.1\n",
            "wurlitzer                                3.1.1\n",
            "xarray                                   2025.11.0\n",
            "xarray-einstats                          0.9.1\n",
            "xgboost                                  3.1.2\n",
            "xlrd                                     2.0.2\n",
            "xxhash                                   3.6.0\n",
            "xyzservices                              2025.11.0\n",
            "yarl                                     1.22.0\n",
            "ydf                                      0.13.0\n",
            "yellowbrick                              1.5\n",
            "yfinance                                 0.2.66\n",
            "zict                                     3.0.0\n",
            "zipp                                     3.23.0\n",
            "zstandard                                0.25.0\n"
          ]
        }
      ],
      "source": [
        "!pip list # Optional to run this cell: To check which of the libraries/packages have been installed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EN235ZNIbMoI",
        "outputId": "6b14cad7-7af4-49c5-8395-11864e9fb401"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transformers: 4.57.3\n",
            "sentence-transformers: 5.1.2\n",
            "torch: 2.9.0+cu126 cuda: True\n"
          ]
        }
      ],
      "source": [
        "# Cell 2: Imports and GPU check: Run this cell after the first cell\n",
        "import os, json, time, math\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Transformers / sentence-transformers\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModel\n",
        "from sentence_transformers import SentenceTransformer, InputExample, losses, evaluation\n",
        "import sentence_transformers # Import the package itself to access __version__\n",
        "\n",
        "# FAISS and BM25\n",
        "import faiss\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "# Datasets and metrics\n",
        "from datasets import load_dataset, Dataset\n",
        "import evaluate\n",
        "import sacrebleu\n",
        "\n",
        "# Print versions and GPU info\n",
        "print(\"transformers:\", transformers.__version__)\n",
        "print(\"sentence-transformers:\", sentence_transformers.__version__)\n",
        "try:\n",
        "    import torch\n",
        "    print(\"torch:\", torch.__version__, \"cuda:\", torch.cuda.is_available())\n",
        "except Exception as e:\n",
        "    print(\"torch not available:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uzv2tu4xZZWX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6197b6e-e1fd-4a93-9005-fcd9833fa7c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded: 60 corpus_clean;  60 passages_min;  100 eval queries\n"
          ]
        }
      ],
      "source": [
        "# Cell 3: Load JSONL/TSV files into Python structures\n",
        "# There will be a content folder on left side bar, files panel. This is our root\n",
        "# folder. Inside it create a data folder, if not already present. Upload all files\n",
        "# there and then run this cell.\n",
        "\n",
        "DATA_DIR = Path(\"drive/MyDrive/data\")  # change if files are elsewhere\n",
        "\n",
        "# Create the data directory if it doesn't exist\n",
        "import os\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "def load_jsonl(path):\n",
        "    items = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                items.append(json.loads(line))\n",
        "    return items\n",
        "\n",
        "corpus_clean = load_jsonl(DATA_DIR / \"urdu_covid_corpus_clean.jsonl\")\n",
        "passages_min = load_jsonl(DATA_DIR / \"urdu_covid_passages_min.jsonl\")\n",
        "# TSV -> list of dicts\n",
        "passages_tsv = []\n",
        "with open(DATA_DIR / \"urdu_covid_passages.tsv\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        # Use split(None, 1) to split on the first occurrence of any whitespace\n",
        "        # This handles cases where the delimiter might be spaces instead of a tab.\n",
        "        if line.strip(): # Ensure line is not empty after stripping whitespace\n",
        "            parts = line.rstrip(\"\\n\").split(None, 1)\n",
        "            if len(parts) == 2:\n",
        "                pid, text = parts\n",
        "                passages_tsv.append({\"id\": pid, \"text\": text})\n",
        "            else:\n",
        "                print(f\"Skipping malformed line in urdu_covid_passages.tsv: {line.strip()}\")\n",
        "\n",
        "eval_queries = load_jsonl(DATA_DIR / \"eval_queries.jsonl\")\n",
        "synthetic_pairs = load_jsonl(DATA_DIR / \"synthetic_qa_pairs.jsonl\")\n",
        "hard_negatives = load_jsonl(DATA_DIR / \"hard_negatives.jsonl\")\n",
        "\n",
        "print(\"Loaded:\", len(corpus_clean), \"corpus_clean; \", len(passages_min), \"passages_min; \", len(eval_queries), \"eval queries\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4Tazxtecpqy",
        "outputId": "26de37f1-965d-41f3-bb54-fc4a0ba1b2e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: File not found: urdu_covid_corpus_clean.jsonl\n",
            "Warning: File not found: urdu_covid_passages_min.jsonl\n",
            "Warning: File not found: eval_queries.jsonl\n",
            "Warning: File not found: synthetic_qa_pairs.jsonl\n",
            "Warning: File not found: hard_negatives.jsonl\n",
            "Warning: File not found: urdu_covid_passages.tsv\n",
            "------------------------------\n",
            "Loaded:\n",
            "  0 corpus_clean\n",
            "  0 passages_min\n",
            "  0 passages_tsv\n",
            "  0 eval queries\n",
            "  0 synthetic pairs\n",
            "  0 hard negatives\n"
          ]
        }
      ],
      "source": [
        "# Cell 3: Load JSONL/TSV files into Python structures\n",
        "# There will be a content folder on left side bar, files panel. This is our root\n",
        "# folder. Inside it create a data folder, if not already present. Upload all files\n",
        "# there and then run this cell.\n",
        "\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# 1. FIX: Point to the current Colab \"content\" folder, not Google Drive\n",
        "DATA_DIR = Path(\"\")  # \".\" represents the current directory (/content in Colab)\n",
        "\n",
        "# Create the data directory if it doesn't exist (optional if you just uploaded them)\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "def load_jsonl(path):\n",
        "    items = []\n",
        "    # Added error handling in case a specific file is missing\n",
        "    if not path.exists():\n",
        "        print(f\"Warning: File not found: {path}\")\n",
        "        return []\n",
        "\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                items.append(json.loads(line))\n",
        "    return items\n",
        "\n",
        "# Load JSONL files\n",
        "corpus_clean = load_jsonl(DATA_DIR / \"urdu_covid_corpus_clean.jsonl\")\n",
        "passages_min = load_jsonl(DATA_DIR / \"urdu_covid_passages_min.jsonl\")\n",
        "eval_queries = load_jsonl(DATA_DIR / \"eval_queries.jsonl\")\n",
        "synthetic_pairs = load_jsonl(DATA_DIR / \"synthetic_qa_pairs.jsonl\")\n",
        "hard_negatives = load_jsonl(DATA_DIR / \"hard_negatives.jsonl\")\n",
        "\n",
        "# TSV -> list of dicts\n",
        "passages_tsv = []\n",
        "tsv_path = DATA_DIR / \"urdu_covid_passages.tsv\"\n",
        "\n",
        "if tsv_path.exists():\n",
        "    with open(tsv_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                # 2. FIX: Standard TSVs split by tab ('\\t').\n",
        "                # If your file strictly uses tabs, use .split('\\t', 1).\n",
        "                # If it might use spaces, keep your .split(None, 1).\n",
        "                # I have kept your logic below, but be aware of this distinction.\n",
        "                parts = line.rstrip(\"\\n\").split(None, 1)\n",
        "\n",
        "                if len(parts) == 2:\n",
        "                    pid, text = parts\n",
        "                    passages_tsv.append({\"id\": pid, \"text\": text})\n",
        "                else:\n",
        "                    print(f\"Skipping malformed line in TSV: {line.strip()}\")\n",
        "else:\n",
        "    print(f\"Warning: File not found: {tsv_path}\")\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(\"Loaded:\")\n",
        "print(f\"  {len(corpus_clean)} corpus_clean\")\n",
        "print(f\"  {len(passages_min)} passages_min\")\n",
        "print(f\"  {len(passages_tsv)} passages_tsv\")\n",
        "print(f\"  {len(eval_queries)} eval queries\")\n",
        "print(f\"  {len(synthetic_pairs)} synthetic pairs\")\n",
        "print(f\"  {len(hard_negatives)} hard negatives\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOOjSvxJcrn-",
        "outputId": "1fe40f84-574f-4674-b77c-39573be075be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing references (should be zero): 0\n"
          ]
        }
      ],
      "source": [
        "# Cell 4: Validate IDs referenced in eval/synthetic/hard_negatives exist in corpus\n",
        "# Run this after Cell 3.\n",
        "passage_ids = {p[\"id\"] for p in passages_min}\n",
        "missing = []\n",
        "for q in eval_queries:\n",
        "    for pid in q.get(\"positive_ids\", []):\n",
        "        if pid not in passage_ids:\n",
        "            missing.append((\"eval\", q[\"query_id\"], pid))\n",
        "for s in synthetic_pairs:\n",
        "    if s[\"positive_id\"] not in passage_ids:\n",
        "        missing.append((\"synthetic\", s[\"synthetic_id\"], s[\"positive_id\"]))\n",
        "for h in hard_negatives:\n",
        "    for pid in h[\"hard_negatives\"]:\n",
        "        if pid not in passage_ids:\n",
        "            missing.append((\"hardneg\", h[\"query_id\"], pid))\n",
        "print(\"Missing references (should be zero):\", len(missing))\n",
        "if missing:\n",
        "    print(missing[:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83StfDLscxFh",
        "outputId": "7b10882b-f884-4960-c65e-1740bb6eba84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BM25 top-3 for sample: [('p0001', 'کورونا وائرس مرض 2019 (COVID-19) ایک متعدی بیماری ہے جس کی عام علامات میں بخار، کھانسی اور سانس لینے میں دشواری شامل ہیں۔', 5.810063974702894), ('p0024', 'بچوں میں کووڈ-19 عام طور پر ہلکا ہوتا ہے مگر بعض نادر معاملات میں شدید علامات سامنے آ سکتی ہیں؛ بچوں کے لیے مخصوص رہنمائی مختلف ہو سکتی ہے۔', 5.103496839739362), ('p0002', 'کووڈ-19 کی تشخیص کے لیے rRT-PCR سویب ٹیسٹ عام طور پر استعمال ہوتے ہیں اور یہ وائرس کی موجودگی کی تصدیق کرتے ہیں۔', 4.589270107579207)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "# Cell 5 (Run after Cell 4): BM25 baseline index (tokenize with simple whitespace; for Urdu this is OK as baseline)\n",
        "# We'll store tokenized corpus and BM25 object for retrieval.\n",
        "from nltk.tokenize import word_tokenize\n",
        "# If nltk not installed, use simple split\n",
        "try:\n",
        "    import nltk\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('punkt_tab') # Added to resolve LookupError for 'punkt_tab'\n",
        "    tokenizer = lambda s: word_tokenize(s)\n",
        "except Exception:\n",
        "    tokenizer = lambda s: s.split()\n",
        "\n",
        "corpus_texts = [p[\"text\"] for p in passages_min]\n",
        "corpus_ids = [p[\"id\"] for p in passages_min]\n",
        "tokenized_corpus = [tokenizer(t) for t in corpus_texts]\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n",
        "\n",
        "# Example retrieval function\n",
        "def bm25_retrieve(query, k=5):\n",
        "    q_tokens = tokenizer(query)\n",
        "    scores = bm25.get_scores(q_tokens)\n",
        "    topk = np.argsort(scores)[::-1][:k]\n",
        "    return [(corpus_ids[i], corpus_texts[i], float(scores[i])) for i in topk]\n",
        "\n",
        "# Quick test\n",
        "print(\"BM25 top-3 for sample:\", bm25_retrieve(eval_queries[0][\"query\"], k=3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmccnrqkczDz",
        "outputId": "b3047162-d025-4fa3-d10e-693de77099f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BM25 retrieval evaluation summary:\n",
            "  n_queries: 100\n",
            "  MRR: 0.8853333333333333\n",
            "  Recall@1: 0.84\n",
            "  Recall@5: 0.95\n",
            "  Precision@1: 0.84\n",
            "  Precision@5: 0.21599999999999964\n",
            "  latency_mean_s: 0.000269017219543457\n",
            "  latency_median_s: 0.00025963783264160156\n",
            "\n",
            "Total misses: 5 / 100. Showing up to 5 misses:\n",
            "Query id: q007 Query: کووڈ-19 ویکسین کا بنیادی مقصد کیا ہے؟\n",
            " Positives: ['p0007']\n",
            " Retrieved top ids: ['p0028', 'p0050', 'p0051', 'p0027', 'p0039']\n",
            "\n",
            "Query id: q019 Query: وینٹیلیشن وبا کے دوران کیوں اہم ہے؟\n",
            " Positives: ['p0020']\n",
            " Retrieved top ids: ['p0017', 'p0060', 'p0031', 'p0048', 'p0027']\n",
            "\n",
            "Query id: q038 Query: ویکسین سائیڈ ایفیکٹس کی نگرانی کیسے کی جاتی ہے؟\n",
            " Positives: ['p0039']\n",
            " Retrieved top ids: ['p0058', 'p0040', 'p0032', 'p0051', 'p0011']\n",
            "\n",
            "Query id: q065 Query: ویکسین کی سائیڈ ایفیکٹس کی رپورٹنگ کیسے ہوتی ہے؟\n",
            " Positives: ['p0039']\n",
            " Retrieved top ids: ['p0058', 'p0047', 'p0032', 'p0022', 'p0025']\n",
            "\n",
            "Query id: q095 Query: وبا کے دوران معاشی بحالی کے لیے کون سے اقدامات کیے جا سکتے ہیں؟\n",
            " Positives: ['p0054']\n",
            " Retrieved top ids: ['p0060', 'p0035', 'p0044', 'p0028', 'p0057']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Cell 5b: BM25-only retriever evaluation tool (run after Cell 5)\n",
        "# Purpose: standalone evaluation harness for the independent BM25 retriever (bm25_retrieve)\n",
        "# Metrics included (applicable to a retriever-only evaluation):\n",
        "#   - Recall@1, Recall@5\n",
        "#   - MRR (Mean Reciprocal Rank)\n",
        "#   - Precision@k (k=1,5)\n",
        "#   - Average / median retrieval latency\n",
        "#   - Optional: match by gold_passage_id or by substring match of gold_answer\n",
        "# Output:\n",
        "#   - Per-query JSONL saved to bm25_eval_results.jsonl\n",
        "#   - Printed summary with all metrics\n",
        "#\n",
        "# Requirements (must be available in the session):\n",
        "#   - bm25_retrieve(query, k) -> list of (passage_id, passage_text, score)\n",
        "#   - eval_queries: list of dicts with at least a query field and optionally:\n",
        "#       * \"question\" or \"query\" or \"q\"  (the query text)\n",
        "#       * \"gold_passage_id\" (optional) OR \"answer\"/\"gold\" (gold text to match)\n",
        "#\n",
        "# Usage:\n",
        "#   - Run this cell after you build the BM25 index (Cell 5).\n",
        "#   - Optionally pass a different eval list or k values to evaluate subsets.\n",
        "\n",
        "# Use this evaluator if your eval_queries items contain \"positive_ids\" and \"gold_answer\"\n",
        "import json, time, re, statistics\n",
        "from typing import List, Dict\n",
        "\n",
        "OUT_JSONL = \"bm25_eval_results.jsonl\"\n",
        "DEFAULT_K = 5\n",
        "RECALL_KS = [1, 5]\n",
        "PRECISION_KS = [1, 5]\n",
        "\n",
        "def normalize_text(s: str) -> str:\n",
        "    if s is None: return \"\"\n",
        "    s = str(s).strip()\n",
        "    return re.sub(r\"\\s+\", \" \", s)\n",
        "\n",
        "def get_query_text(item: Dict) -> str:\n",
        "    return item.get(\"query\") or item.get(\"question\") or item.get(\"q\") or \"\"\n",
        "\n",
        "def evaluate_bm25_with_positive_ids(eval_items: List[Dict],\n",
        "                                    out_jsonl: str = OUT_JSONL,\n",
        "                                    k: int = DEFAULT_K,\n",
        "                                    recall_ks = RECALL_KS,\n",
        "                                    precision_ks = PRECISION_KS):\n",
        "    per_query = []\n",
        "    latencies = []\n",
        "    rr_list = []\n",
        "    recall_counts = {rk: 0 for rk in recall_ks}\n",
        "    precision_sums = {pk: 0.0 for pk in precision_ks}\n",
        "    total = 0\n",
        "\n",
        "    for item in eval_items:\n",
        "        total += 1\n",
        "        q = get_query_text(item)\n",
        "        positive_ids = item.get(\"positive_ids\") or item.get(\"positive_id\") or []\n",
        "        # normalize to list of strings\n",
        "        if isinstance(positive_ids, str):\n",
        "            positive_ids = [positive_ids]\n",
        "        positive_ids = [str(x) for x in positive_ids]\n",
        "\n",
        "        gold_text = normalize_text(item.get(\"gold_answer\") or item.get(\"answer\") or \"\")\n",
        "\n",
        "        t0 = time.time()\n",
        "        try:\n",
        "            hits = bm25_retrieve(q, k=k)   # (id, text, score)\n",
        "        except Exception as e:\n",
        "            hits = []\n",
        "            print(f\"[eval] bm25_retrieve error for query {q[:60]}... -> {e}\")\n",
        "        latency = time.time() - t0\n",
        "        latencies.append(latency)\n",
        "\n",
        "        retrieved_ids = [h[0] for h in hits]\n",
        "        retrieved_texts = [h[1] for h in hits]\n",
        "\n",
        "        # Reciprocal rank: first position among positives\n",
        "        rr = 0.0\n",
        "        for rank, pid in enumerate(retrieved_ids, start=1):\n",
        "            if pid in positive_ids:\n",
        "                rr = 1.0 / rank\n",
        "                break\n",
        "        rr_list.append(rr)\n",
        "\n",
        "        # Recall@k and Precision@k (multiple positives supported)\n",
        "        for rk in recall_ks:\n",
        "            recall_counts[rk] += 1 if any(pid in positive_ids for pid in retrieved_ids[:rk]) else 0\n",
        "        for pk in precision_ks:\n",
        "            # precision@k = (# positives in top-k) / k\n",
        "            num_pos_in_topk = sum(1 for pid in retrieved_ids[:pk] if pid in positive_ids)\n",
        "            precision_sums[pk] += (num_pos_in_topk / pk)\n",
        "\n",
        "        per_query.append({\n",
        "            \"query_id\": item.get(\"query_id\"),\n",
        "            \"query\": q,\n",
        "            \"positive_ids\": positive_ids,\n",
        "            \"gold_text\": gold_text,\n",
        "            \"retrieved_ids\": retrieved_ids,\n",
        "            \"retrieved_texts_preview\": [t[:300] for t in retrieved_texts],\n",
        "            \"reciprocal_rank\": rr,\n",
        "            \"latency\": latency\n",
        "        })\n",
        "\n",
        "    n = total if total else 1\n",
        "    mrr = sum(rr_list) / n\n",
        "    recall_at = {rk: recall_counts[rk] / n for rk in recall_ks}\n",
        "    precision_at = {pk: precision_sums[pk] / n for pk in precision_ks}\n",
        "    latency_mean = statistics.mean(latencies) if latencies else 0.0\n",
        "    latency_median = statistics.median(latencies) if latencies else 0.0\n",
        "\n",
        "    summary = {\n",
        "        \"n_queries\": n,\n",
        "        \"MRR\": mrr,\n",
        "        **{f\"Recall@{rk}\": recall_at[rk] for rk in recall_ks},\n",
        "        **{f\"Precision@{pk}\": precision_at[pk] for pk in precision_ks},\n",
        "        \"latency_mean_s\": latency_mean,\n",
        "        \"latency_median_s\": latency_median\n",
        "    }\n",
        "\n",
        "    with open(out_jsonl, \"w\", encoding=\"utf-8\") as f:\n",
        "        for r in per_query:\n",
        "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    return summary, per_query\n",
        "\n",
        "# Run it\n",
        "if 'eval_queries' not in globals():\n",
        "    # try to load from file if not in memory\n",
        "    eval_queries = []\n",
        "    with open(\"eval_queries.jsonl\",\"r\",encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            eval_queries.append(json.loads(line))\n",
        "\n",
        "summary, records = evaluate_bm25_with_positive_ids(eval_queries, out_jsonl=OUT_JSONL, k=DEFAULT_K)\n",
        "print(\"BM25 retrieval evaluation summary:\")\n",
        "for k,v in summary.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "# show a few examples where retrieval missed positives\n",
        "misses = [r for r in records if r[\"reciprocal_rank\"] == 0.0]\n",
        "print(f\"\\nTotal misses: {len(misses)} / {len(records)}. Showing up to 5 misses:\")\n",
        "for r in misses[:5]:\n",
        "    print(\"Query id:\", r.get(\"query_id\"), \"Query:\", r[\"query\"][:80])\n",
        "    print(\" Positives:\", r[\"positive_ids\"])\n",
        "    print(\" Retrieved top ids:\", r[\"retrieved_ids\"][:8])\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "64e9b46e537e473e907a852f8ff12847",
            "166054e3327b44fea0e8def95d6c3e32",
            "56711875ce7045f589d4995366c5b59b",
            "33f2dc7df44244898ef3f713880b1db7",
            "315e019b86ad4d72894df4e89f82521e",
            "fdabaf8bb6784393903feb5cea3b6e26",
            "e699628a6d0c4b139681f5e3d1edf25f",
            "51230ddbfe43464ca4e56993ed899fc0",
            "13ded27e65144607bc2274b384afba6f",
            "59b41823c7ff4b3abf0ea283a7ea1ac0",
            "b9012b4982b34a05a74b6e08a848b67d"
          ]
        },
        "id": "hpexUaHYc03R",
        "outputId": "de27c598-dfb5-48ab-ae91-5dae45a5ff49"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "64e9b46e537e473e907a852f8ff12847"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dense top-3: [('p0024', 'بچوں میں کووڈ-19 عام طور پر ہلکا ہوتا ہے مگر بعض نادر معاملات میں شدید علامات سامنے آ سکتی ہیں؛ بچوں کے لیے مخصوص رہنمائی مختلف ہو سکتی ہے۔', 0.7648559808731079), ('p0021', 'کووڈ-19 کے بعد بعض افراد میں طویل مدتی علامات (Long COVID) جیسے تھکن، سانس کی تکلیف اور دماغی دھند برقرار رہ سکتی ہیں؛ ریہیب پروگرامز مدد دیتے ہیں۔', 0.6735702753067017), ('p0036', 'کووڈ-19 کے مریضوں میں خون جمنے کے مسائل اور دیگر پیچیدگیاں بعض اوقات سامنے آئیں، اس لیے طبی نگرانی اور مناسب علاج ضروری ہے۔', 0.6472983360290527)]\n"
          ]
        }
      ],
      "source": [
        "# Cell 6: Dense embeddings with a multilingual model (use a compact model for Colab)\n",
        "# We use a multilingual SBERT model that supports Urdu reasonably (e.g., 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
        "embed_model_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "embedder = SentenceTransformer(embed_model_name)\n",
        "\n",
        "# Compute embeddings for passages_min (batching)\n",
        "passage_embeddings = embedder.encode(corpus_texts, show_progress_bar=True, convert_to_numpy=True)\n",
        "\n",
        "# Build FAISS index (cosine similarity via normalized vectors)\n",
        "d = passage_embeddings.shape[1]\n",
        "index = faiss.IndexFlatIP(d)  # inner product\n",
        "# normalize embeddings for cosine\n",
        "faiss.normalize_L2(passage_embeddings)\n",
        "index.add(passage_embeddings)\n",
        "\n",
        "# Map index positions to ids\n",
        "# retrieval function\n",
        "def dense_retrieve(query, k=5):\n",
        "    q_emb = embedder.encode([query], convert_to_numpy=True)\n",
        "    faiss.normalize_L2(q_emb)\n",
        "    D, I = index.search(q_emb, k)\n",
        "    results = []\n",
        "    for idx, score in zip(I[0], D[0]):\n",
        "        results.append((corpus_ids[idx], corpus_texts[idx], float(score)))\n",
        "    return results\n",
        "\n",
        "# Quick test\n",
        "print(\"Dense top-3:\", dense_retrieve(eval_queries[0][\"query\"], k=3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aX-IFq-Wc2tF",
        "outputId": "cdf270de-0f7b-4d8a-eaeb-8d9e553b63ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[dense_eval] Running dense retriever evaluation...\n",
            "\n",
            "Dense retriever evaluation summary:\n",
            "  n_queries: 100\n",
            "  MRR: 0.8490000000000001\n",
            "  Recall@1: 0.77\n",
            "  Recall@5: 0.96\n",
            "  Precision@1: 0.77\n",
            "  Precision@5: 0.21599999999999964\n",
            "  latency_mean_s: 0.0102130389213562\n",
            "  latency_median_s: 0.00966346263885498\n",
            "\n",
            "Examples (first 5):\n",
            " - Query: کووڈ-19 کی عام علامات کیا ہیں؟\n",
            "   Retrieved ids: ['p0024', 'p0021', 'p0001', 'p0036', 'p0044']\n",
            "   Reciprocal rank: 0.3333333333333333 Latency(s): 0.0139\n",
            "\n",
            " - Query: کووڈ-19 کی تشخیص کے لیے کون سا ٹیسٹ عام طور پر استعمال ہوتا ہے؟\n",
            "   Retrieved ids: ['p0002', 'p0018', 'p0036', 'p0045', 'p0024']\n",
            "   Reciprocal rank: 1.0 Latency(s): 0.0106\n",
            "\n",
            " - Query: ہاتھوں کی صفائی وبا کے دوران کیوں ضروری ہے؟\n",
            "   Retrieved ids: ['p0030', 'p0003', 'p0042', 'p0020', 'p0041']\n",
            "   Reciprocal rank: 0.5 Latency(s): 0.0099\n",
            "\n",
            " - Query: ماسک پہننے کے کیا فوائد ہیں؟\n",
            "   Retrieved ids: ['p0004', 'p0029', 'p0022', 'p0042', 'p0050']\n",
            "   Reciprocal rank: 1.0 Latency(s): 0.0094\n",
            "\n",
            " - Query: سماجی فاصلہ رکھنے کی اہمیت کیا ہے؟\n",
            "   Retrieved ids: ['p0005', 'p0054', 'p0016', 'p0015', 'p0043']\n",
            "   Reciprocal rank: 1.0 Latency(s): 0.009\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Cell 6b: Evaluation of dense retriever (run after Cell 6)\n",
        "# Purpose: measure Recall@1, Recall@5, MRR, Precision@k, latency for dense_retrieve\n",
        "# Uses eval_queries with \"positive_ids\" and \"gold_answer\" fields\n",
        "\n",
        "import json, time, re, statistics\n",
        "\n",
        "OUT_JSONL_DENSE = \"dense_eval_results.jsonl\"\n",
        "DEFAULT_K = 5\n",
        "RECALL_KS = [1, 5]\n",
        "PRECISION_KS = [1, 5]\n",
        "\n",
        "def normalize_text(s):\n",
        "    if s is None: return \"\"\n",
        "    return re.sub(r\"\\s+\", \" \", str(s).strip())\n",
        "\n",
        "def get_query_text(item):\n",
        "    return item.get(\"query\") or item.get(\"question\") or item.get(\"q\") or \"\"\n",
        "\n",
        "def evaluate_dense(eval_items, out_jsonl=OUT_JSONL_DENSE, k=DEFAULT_K,\n",
        "                   recall_ks=RECALL_KS, precision_ks=PRECISION_KS):\n",
        "    per_query = []\n",
        "    latencies, rr_list = [], []\n",
        "    recall_counts = {rk: 0 for rk in recall_ks}\n",
        "    precision_sums = {pk: 0.0 for pk in precision_ks}\n",
        "    total = 0\n",
        "\n",
        "    for item in eval_items:\n",
        "        total += 1\n",
        "        q = get_query_text(item)\n",
        "        pos_ids = item.get(\"positive_ids\") or []\n",
        "        if isinstance(pos_ids, str): pos_ids = [pos_ids]\n",
        "        pos_ids = [str(x) for x in pos_ids]\n",
        "\n",
        "        gold_text = normalize_text(item.get(\"gold_answer\") or \"\")\n",
        "\n",
        "        t0 = time.time()\n",
        "        hits = dense_retrieve(q, k=k)  # (id, text, score)\n",
        "        latency = time.time() - t0\n",
        "        latencies.append(latency)\n",
        "\n",
        "        retrieved_ids = [h[0] for h in hits]\n",
        "        retrieved_texts = [h[1] for h in hits]\n",
        "\n",
        "        # Reciprocal rank\n",
        "        rr = 0.0\n",
        "        for rank, pid in enumerate(retrieved_ids, start=1):\n",
        "            if pid in pos_ids:\n",
        "                rr = 1.0 / rank\n",
        "                break\n",
        "        rr_list.append(rr)\n",
        "\n",
        "        # Recall@k and Precision@k\n",
        "        for rk in recall_ks:\n",
        "            recall_counts[rk] += 1 if any(pid in pos_ids for pid in retrieved_ids[:rk]) else 0\n",
        "        for pk in precision_ks:\n",
        "            num_pos_in_topk = sum(1 for pid in retrieved_ids[:pk] if pid in pos_ids)\n",
        "            precision_sums[pk] += num_pos_in_topk / pk\n",
        "\n",
        "        per_query.append({\n",
        "            \"query_id\": item.get(\"query_id\"),\n",
        "            \"query\": q,\n",
        "            \"positive_ids\": pos_ids,\n",
        "            \"gold_text\": gold_text,\n",
        "            \"retrieved_ids\": retrieved_ids,\n",
        "            \"retrieved_texts_preview\": [txt[:300] for txt in retrieved_texts],\n",
        "            \"reciprocal_rank\": rr,\n",
        "            \"latency\": latency\n",
        "        })\n",
        "\n",
        "    n = total if total else 1\n",
        "    summary = {\n",
        "        \"n_queries\": n,\n",
        "        \"MRR\": sum(rr_list)/n,\n",
        "        **{f\"Recall@{rk}\": recall_counts[rk]/n for rk in recall_ks},\n",
        "        **{f\"Precision@{pk}\": precision_sums[pk]/n for pk in precision_ks},\n",
        "        \"latency_mean_s\": statistics.mean(latencies) if latencies else 0.0,\n",
        "        \"latency_median_s\": statistics.median(latencies) if latencies else 0.0\n",
        "    }\n",
        "\n",
        "    with open(out_jsonl, \"w\", encoding=\"utf-8\") as f:\n",
        "        for r in per_query:\n",
        "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    return summary, per_query\n",
        "\n",
        "# Run evaluation\n",
        "print(\"[dense_eval] Running dense retriever evaluation...\")\n",
        "summary_dense, records_dense = evaluate_dense(eval_queries, out_jsonl=OUT_JSONL_DENSE, k=DEFAULT_K)\n",
        "print(\"\\nDense retriever evaluation summary:\")\n",
        "for k,v in summary_dense.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "# Show a few examples\n",
        "print(\"\\nExamples (first 5):\")\n",
        "for r in records_dense[:5]:\n",
        "    print(\" - Query:\", r[\"query\"][:80])\n",
        "    print(\"   Retrieved ids:\", r[\"retrieved_ids\"][:6])\n",
        "    print(\"   Reciprocal rank:\", r[\"reciprocal_rank\"], \"Latency(s):\", round(r[\"latency\"], 4))\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7lxK5E-c5AR",
        "outputId": "a5aa1cc2-b285-4067-d40c-bb41000d85dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared 500 triplet examples.\n",
            "Train examples: 400\n",
            "Validation examples: 100\n"
          ]
        }
      ],
      "source": [
        "# Cell 7: Prepare InputExamples for sentence-transformers fine-tuning\n",
        "# Now with an 80/20 train/validation split\n",
        "\n",
        "from sentence_transformers import InputExample\n",
        "import random\n",
        "\n",
        "pid2text = {p[\"id\"]: p[\"text\"] for p in passages_min}\n",
        "\n",
        "examples = []\n",
        "for s in synthetic_pairs:\n",
        "    q = s[\"query\"]\n",
        "    pos = pid2text.get(s[\"positive_id\"])\n",
        "    neg = None\n",
        "    # Find hard negatives if available\n",
        "    hn = next((h for h in hard_negatives if h[\"query_id\"] == s.get(\"synthetic_id\", s.get(\"query_id\"))), None)\n",
        "    if hn:\n",
        "        for nid in hn[\"hard_negatives\"]:\n",
        "            if nid != s[\"positive_id\"]:\n",
        "                neg = pid2text.get(nid)\n",
        "                break\n",
        "    if neg is None:\n",
        "        # fallback: random negative\n",
        "        neg_id = random.choice([pid for pid in corpus_ids if pid != s[\"positive_id\"]])\n",
        "        neg = pid2text[neg_id]\n",
        "    if pos and neg:\n",
        "        examples.append(InputExample(texts=[q, pos, neg]))\n",
        "\n",
        "print(\"Prepared\", len(examples), \"triplet examples.\")\n",
        "\n",
        "# --- Split into train/validation (80/20) ---\n",
        "random.shuffle(examples)\n",
        "split_idx = int(0.8 * len(examples))\n",
        "train_examples = examples[:split_idx]\n",
        "val_examples = examples[split_idx:]\n",
        "\n",
        "print(\"Train examples:\", len(train_examples))\n",
        "print(\"Validation examples:\", len(val_examples))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279,
          "referenced_widgets": [
            "7a74fad46ccf440b82cd0891e86ae75d",
            "d6d9413b13294076896a94f376eb255e",
            "85a451287f5344e1a2de3fff7a7ec74f",
            "9fa1256a64b7496cabafea67e9c013ff",
            "08da18dbf6e0438ca4ff066acd85a42a",
            "28ca07379bad4f4bb151fec425a6e344",
            "1267d19086c642bfa6e6807461170c1e",
            "73f7469c86b24eaaa7440128813fa724",
            "ef3dba37e11e4216b486b23c046f333c",
            "fdd486dcd2fe4c5c9bd897d375fae94a",
            "9c02984973bb43ffa6fce2ff7d427bd7"
          ]
        },
        "id": "_-SEj6lLc61H",
        "outputId": "ee4d3b19-31a1-49f4-ec11-03c103281a98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting fine-tuning (WandB Disabled)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7a74fad46ccf440b82cd0891e86ae75d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 00:15, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Val Ir Passages Cosine Accuracy@1</th>\n",
              "      <th>Val Ir Passages Cosine Accuracy@3</th>\n",
              "      <th>Val Ir Passages Cosine Accuracy@5</th>\n",
              "      <th>Val Ir Passages Cosine Accuracy@10</th>\n",
              "      <th>Val Ir Passages Cosine Precision@1</th>\n",
              "      <th>Val Ir Passages Cosine Precision@3</th>\n",
              "      <th>Val Ir Passages Cosine Precision@5</th>\n",
              "      <th>Val Ir Passages Cosine Precision@10</th>\n",
              "      <th>Val Ir Passages Cosine Recall@1</th>\n",
              "      <th>Val Ir Passages Cosine Recall@3</th>\n",
              "      <th>Val Ir Passages Cosine Recall@5</th>\n",
              "      <th>Val Ir Passages Cosine Recall@10</th>\n",
              "      <th>Val Ir Passages Cosine Ndcg@10</th>\n",
              "      <th>Val Ir Passages Cosine Mrr@10</th>\n",
              "      <th>Val Ir Passages Cosine Map@100</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.383333</td>\n",
              "      <td>0.240000</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.575000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.775000</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.810544</td>\n",
              "      <td>0.865476</td>\n",
              "      <td>0.761609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.850000</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.850000</td>\n",
              "      <td>0.383333</td>\n",
              "      <td>0.260000</td>\n",
              "      <td>0.135000</td>\n",
              "      <td>0.575000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.825000</td>\n",
              "      <td>0.850000</td>\n",
              "      <td>0.808619</td>\n",
              "      <td>0.895000</td>\n",
              "      <td>0.762581</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fine-tuning complete. Using in-memory fine-tuned 'embedder' (no reload).\n"
          ]
        }
      ],
      "source": [
        "# Cell 8 (use in-memory model; do NOT reload): Fine-tune SBERT with triplet loss and IR validation on passages_min\n",
        "import os\n",
        "# --- GRANDMASTER FIX: DISABLE WANDB ---\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
        "# --------------------------------------\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from sentence_transformers import SentenceTransformer, losses, evaluation\n",
        "#import faiss\n",
        "\n",
        "# Sanity checks\n",
        "assert isinstance(train_examples, list) and len(train_examples) > 0, \"train_examples must be a non-empty list\"\n",
        "assert 'passages_min' in globals(), \"passages_min must be loaded\"\n",
        "assert 'eval_queries' in globals(), \"eval_queries must be loaded\"\n",
        "\n",
        "# Build validation split against real corpus & labels\n",
        "# (Check if eval_queries_val exists, otherwise split eval_queries)\n",
        "eval_val = eval_queries_val if 'eval_queries_val' in globals() else eval_queries[int(0.8*len(eval_queries)):]\n",
        "\n",
        "val_queries_dict = {it[\"query_id\"]: it[\"query\"] for it in eval_val}\n",
        "# Fix: Ensure positive_ids is a list\n",
        "val_relevant_dict = {it[\"query_id\"]: set(it[\"positive_ids\"] if isinstance(it[\"positive_ids\"], list) else [it[\"positive_ids\"]]) for it in eval_val}\n",
        "val_corpus_dict = {p[\"id\"]: p[\"text\"] for p in passages_min}\n",
        "\n",
        "# Warn if labels reference missing ids\n",
        "missing = []\n",
        "for qid, rels in val_relevant_dict.items():\n",
        "    for pid in rels:\n",
        "        if pid not in val_corpus_dict:\n",
        "            missing.append((qid, pid))\n",
        "if missing:\n",
        "    print(f\"Warning: {len(missing)} relevant ids not found in corpus. Example:\", missing[:3])\n",
        "\n",
        "# Construct evaluator (defaults to cosine similarity)\n",
        "retrieval_evaluator = evaluation.InformationRetrievalEvaluator(\n",
        "    queries=val_queries_dict,\n",
        "    corpus=val_corpus_dict,\n",
        "    relevant_docs=val_relevant_dict,\n",
        "    name=\"val_ir_passages\"\n",
        ")\n",
        "\n",
        "# Start from baseline multilingual MiniLM\n",
        "# We use the variable 'embedder' from Cell 6 to ensure we continue correctly\n",
        "if 'embedder' not in globals():\n",
        "    embedder = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "embedder.to(\"cuda\")\n",
        "\n",
        "# Triplet loss with conservative settings\n",
        "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
        "train_loss = losses.TripletLoss(\n",
        "    model=embedder,\n",
        "    distance_metric=losses.TripletDistanceMetric.COSINE,\n",
        "    triplet_margin=0.3\n",
        ")\n",
        "\n",
        "num_epochs = 2\n",
        "warmup_steps = int(len(train_dataloader) * num_epochs * 0.1)\n",
        "optimizer_params = {'lr': 2e-5}\n",
        "\n",
        "print(\"Starting fine-tuning (WandB Disabled)...\")\n",
        "\n",
        "# Train with IR evaluator\n",
        "embedder.fit(\n",
        "    train_objectives=[(train_dataloader, train_loss)],\n",
        "    evaluator=retrieval_evaluator,\n",
        "    epochs=num_epochs,\n",
        "    warmup_steps=warmup_steps,\n",
        "    optimizer_params=optimizer_params,\n",
        "    show_progress_bar=True,\n",
        "    output_path=\"fine_tuned_sbert_urdu_passages\"\n",
        ")\n",
        "\n",
        "print(\"✅ Fine-tuning complete. Using in-memory fine-tuned 'embedder' (no reload).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAu5qjmwc9zl",
        "outputId": "3d1a0916-dc24-4f6e-c8e8-ba69346b35cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saving model to /content/drive/MyDrive/models/urdu_dense_retriever_best ...\n",
            "✅ Model saved! You can now use Cell 8c in future sessions to skip training.\n"
          ]
        }
      ],
      "source": [
        "# Cell 8b: Save the Fine-Tuned Model to Drive (Run ONLY if satisfied with accuracy)\n",
        "import os\n",
        "\n",
        "# Define path\n",
        "MODEL_SAVE_PATH = \"/content/drive/MyDrive/models/urdu_dense_retriever_best\"\n",
        "\n",
        "print(f\"💾 Saving model to {MODEL_SAVE_PATH} ...\")\n",
        "\n",
        "# Create directory if not exists\n",
        "os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
        "\n",
        "# Save the model\n",
        "embedder.save(MODEL_SAVE_PATH)\n",
        "\n",
        "print(f\"✅ Model saved! You can now use Cell 8c in future sessions to skip training.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaknzmOqG0R9"
      },
      "outputs": [],
      "source": [
        "# Cell 8c: FAST START - Load Model from Drive & Rebuild FAISS (Skips Training)\n",
        "# Run this INSTEAD of Cells 6, 7, 8, 8b in future sessions.\n",
        "\n",
        "import os\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "MODEL_SAVE_PATH = \"/content/drive/MyDrive/models/urdu_dense_retriever_best\"\n",
        "\n",
        "# 1. Load the Model\n",
        "if os.path.exists(MODEL_SAVE_PATH):\n",
        "    print(f\"📂 Loading saved model from: {MODEL_SAVE_PATH}\")\n",
        "    embedder = SentenceTransformer(MODEL_SAVE_PATH).to(\"cuda\")\n",
        "    print(\"✅ Model loaded successfully.\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"❌ No saved model found at {MODEL_SAVE_PATH}. Please run Cell 8 & 8b once to create it!\")\n",
        "\n",
        "# 2. Rebuild FAISS Index (Critical Step)\n",
        "# We must re-encode the corpus because we just loaded a specific model\n",
        "print(\"⏳ Generating embeddings for corpus...\")\n",
        "corpus_texts = [p[\"text\"] for p in passages_min]\n",
        "\n",
        "# Generate embeddings\n",
        "passage_embeddings = embedder.encode(corpus_texts, show_progress_bar=True, convert_to_numpy=True)\n",
        "\n",
        "# Build FAISS\n",
        "faiss.normalize_L2(passage_embeddings)\n",
        "index = faiss.IndexFlatIP(passage_embeddings.shape[1])\n",
        "index.add(passage_embeddings)\n",
        "\n",
        "# 3. Define the Retrieval Function\n",
        "# (We must re-define this here because we skipped the previous cells that defined it)\n",
        "def dense_retrieve(query, k=5):\n",
        "    q_emb = embedder.encode([query], convert_to_numpy=True)\n",
        "    faiss.normalize_L2(q_emb)\n",
        "    D, I = index.search(q_emb, k)\n",
        "    results = []\n",
        "    for idx, score in zip(I[0], D[0]):\n",
        "        results.append((corpus_ids[idx], corpus_texts[idx], float(score)))\n",
        "    return results\n",
        "\n",
        "print(\"✅ Dense Retriever System Restored & Ready for Hybrid Fusion (Cell 9).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwDU_Bm6dAlp",
        "outputId": "33aa0b45-9680-4a85-93d8-606e8da5a98f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample dense top-5 ids: ['p0024', 'p0021', 'p0001', 'p0036', 'p0044']\n",
            "Sample bm25_new top-5 ids: ['p0001', 'p0024', 'p0002', 'p0044', 'p0021']\n",
            "Sample hybrid_score top-5 ids: ['p0024', 'p0001', 'p0021', 'p0044', 'p0002']\n",
            "Sample hybrid_rrf top-5 ids: ['p0024', 'p0001', 'p0021', 'p0044', 'p0002']\n"
          ]
        }
      ],
      "source": [
        "# Cell 9 (final): Retriever wrapper with true fusion modes (non-destructive)\n",
        "# - Creates bm25_new only if not present\n",
        "# - Supports modes: 'bm25', 'dense', 'hybrid_interleave' (legacy), 'hybrid_score', 'hybrid_rrf'\n",
        "# - Returns list of (pid, text, score) tuples\n",
        "# - Does NOT rebuild or overwrite dense/FAISS objects\n",
        "\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# ---------- Config ----------\n",
        "# Tune these later on a small validation set\n",
        "DEFAULT_RETRIEVE_POOL = 50\n",
        "SCORE_FUSION_ALPHA = 0.6   # alpha in [0,1] for score fusion: alpha * dense + (1-alpha) * bm25\n",
        "RRF_K = 60                 # reciprocal rank fusion constant\n",
        "\n",
        "# ---------- Sanity checks for canonical corpus ----------\n",
        "assert 'passages_min' in globals() and isinstance(passages_min, list) and len(passages_min) > 0, \"passages_min must be loaded\"\n",
        "assert 'pid2text' in globals() and isinstance(pid2text, dict) and len(pid2text) > 0, \"pid2text must be available\"\n",
        "assert 'dense_retrieve' in globals(), \"dense_retrieve wrapper must be defined (fine-tuned dense retriever)\"\n",
        "\n",
        "# ---------- Build or reuse BM25 index (non-destructive) ----------\n",
        "try:\n",
        "    # If bm25_new already exists from a previous run, reuse it\n",
        "    bm25_new  # noqa: F821\n",
        "except Exception:\n",
        "    try:\n",
        "        from rank_bm25 import BM25Okapi\n",
        "    except Exception:\n",
        "        import sys, subprocess\n",
        "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"rank_bm25\"], check=True)\n",
        "        from rank_bm25 import BM25Okapi\n",
        "\n",
        "    # Build tokenized corpus from passages_min (light normalization)\n",
        "    def _normalize_for_bm25(s: str) -> str:\n",
        "        if s is None:\n",
        "            return \"\"\n",
        "        s = s.replace(\"\\u200c\", \" \")  # zero-width non-joiner\n",
        "        s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "        return s.lower()\n",
        "\n",
        "    bm25_new_ids = [p[\"id\"] for p in passages_min]\n",
        "    bm25_new_texts = [p[\"text\"] for p in passages_min]\n",
        "    bm25_new_tokenized = [_normalize_for_bm25(t).split() for t in bm25_new_texts]\n",
        "    bm25_new = BM25Okapi(bm25_new_tokenized)\n",
        "\n",
        "# Safe wrapper for BM25 that returns (pid, text, score)\n",
        "def bm25_new_retrieve(query: str, k: int = 5):\n",
        "    q_tok = _normalize_for_bm25(query).split()\n",
        "    scores = bm25_new.get_scores(q_tok)\n",
        "    top_idx = np.argsort(scores)[::-1][:k]\n",
        "    results = []\n",
        "    for i in top_idx:\n",
        "        i = int(i)\n",
        "        pid = bm25_new_ids[i]\n",
        "        text = bm25_new_texts[i]\n",
        "        score = float(scores[i])\n",
        "        results.append((pid, text, score))\n",
        "    return results\n",
        "\n",
        "# ---------- Fusion utilities ----------\n",
        "def normalize_scores(score_map):\n",
        "    \"\"\"Min-max normalize a dict of scores to [0,1].\"\"\"\n",
        "    if not score_map:\n",
        "        return {}\n",
        "    vals = list(score_map.values())\n",
        "    lo, hi = min(vals), max(vals)\n",
        "    if hi == lo:\n",
        "        return {k: 1.0 for k in score_map}\n",
        "    return {k: (v - lo) / (hi - lo) for k, v in score_map.items()}\n",
        "\n",
        "def rrf_rank(dense_list, bm25_list, k_rrf=RRF_K):\n",
        "    \"\"\"Reciprocal Rank Fusion: returns sorted list of pids by RRF score.\"\"\"\n",
        "    score = {}\n",
        "    for rank, (pid, _, _) in enumerate(dense_list, start=1):\n",
        "        score[pid] = score.get(pid, 0.0) + 1.0 / (k_rrf + rank)\n",
        "    for rank, (pid, _, _) in enumerate(bm25_list, start=1):\n",
        "        score[pid] = score.get(pid, 0.0) + 1.0 / (k_rrf + rank)\n",
        "    sorted_pids = sorted(score.keys(), key=lambda p: score[p], reverse=True)\n",
        "    return sorted_pids, score\n",
        "\n",
        "# ---------- Metadata filter helper (unchanged semantics) ----------\n",
        "# If you have meta_map from corpus_clean, it will be used; otherwise fallback to passages_min metadata\n",
        "if 'corpus_clean' in globals():\n",
        "    meta_map = {p[\"id\"]: p for p in corpus_clean}\n",
        "else:\n",
        "    meta_map = {p[\"id\"]: p for p in passages_min}\n",
        "\n",
        "def filter_by_metadata(candidate_ids, min_date=None, max_date=None, allowed_sources=None, exclude_time_sensitive=None):\n",
        "    out = []\n",
        "    for pid in candidate_ids:\n",
        "        m = meta_map.get(pid, {})\n",
        "        ok = True\n",
        "        if min_date or max_date:\n",
        "            dt = None\n",
        "            if \"retrieved_at\" in m:\n",
        "                try:\n",
        "                    dt = datetime.fromisoformat(m[\"retrieved_at\"])\n",
        "                except Exception:\n",
        "                    dt = None\n",
        "            if dt:\n",
        "                if min_date and dt < min_date: ok = False\n",
        "                if max_date and dt > max_date: ok = False\n",
        "        if allowed_sources and m.get(\"source\") not in allowed_sources:\n",
        "            ok = False\n",
        "        if exclude_time_sensitive is not None and m.get(\"time_sensitive\") == exclude_time_sensitive:\n",
        "            ok = False\n",
        "        if ok:\n",
        "            out.append(pid)\n",
        "    return out\n",
        "\n",
        "# ---------- Main retrieve wrapper with fusion modes ----------\n",
        "def retrieve(query: str, k: int = 5, mode: str = \"hybrid_score\", min_date=None, max_date=None, allowed_sources=None, exclude_time_sensitive=None):\n",
        "    \"\"\"\n",
        "    retrieve(query, k, mode)\n",
        "    Modes:\n",
        "      - 'bm25' : BM25-only (bm25_new_retrieve)\n",
        "      - 'dense' : dense-only (dense_retrieve)\n",
        "      - 'hybrid_interleave' : legacy interleave (dense first, then bm25)\n",
        "      - 'hybrid_score' : score fusion (normalized dense + bm25)\n",
        "      - 'hybrid_rrf' : reciprocal rank fusion (RRF)\n",
        "    Returns: list of (pid, text, score)\n",
        "    \"\"\"\n",
        "    # Get candidate pools (pool size configurable)\n",
        "    pool = max(DEFAULT_RETRIEVE_POOL, k)\n",
        "    dense_hits = dense_retrieve(query, k=pool)   # expected (pid, text, score)\n",
        "    bm25_hits = bm25_new_retrieve(query, k=pool) # (pid, text, score)\n",
        "\n",
        "    # Mode-specific behavior\n",
        "    if mode == \"bm25\":\n",
        "        results = bm25_hits[:k]\n",
        "    elif mode == \"dense\":\n",
        "        results = dense_hits[:k]\n",
        "    elif mode == \"hybrid_interleave\":\n",
        "        # preserve dense-first interleaving (legacy behavior)\n",
        "        seen = set()\n",
        "        cands = []\n",
        "        for lst in (dense_hits, bm25_hits):\n",
        "            for pid, text, score in lst:\n",
        "                if pid not in seen:\n",
        "                    seen.add(pid)\n",
        "                    cands.append((pid, text, float(score)))\n",
        "        results = cands[:k]\n",
        "    elif mode == \"hybrid_score\":\n",
        "        # Score fusion: normalize and combine\n",
        "        dense_scores = {pid: sc for pid, _, sc in dense_hits}\n",
        "        bm25_scores = {pid: sc for pid, _, sc in bm25_hits}\n",
        "        d_norm = normalize_scores(dense_scores)\n",
        "        b_norm = normalize_scores(bm25_scores)\n",
        "        alpha = SCORE_FUSION_ALPHA\n",
        "        combined = {}\n",
        "        for pid in set(list(d_norm.keys()) + list(b_norm.keys())):\n",
        "            combined[pid] = alpha * d_norm.get(pid, 0.0) + (1 - alpha) * b_norm.get(pid, 0.0)\n",
        "        # sort by combined score\n",
        "        sorted_pids = sorted(combined.keys(), key=lambda p: combined[p], reverse=True)\n",
        "        results = []\n",
        "        for pid in sorted_pids[:k]:\n",
        "            text = pid2text.get(pid, next((p[\"text\"] for p in passages_min if p[\"id\"] == pid), \"\"))\n",
        "            results.append((pid, text, float(combined[pid])))\n",
        "    elif mode == \"hybrid_rrf\":\n",
        "        sorted_pids, score_map = rrf_rank(dense_hits, bm25_hits, k_rrf=RRF_K)\n",
        "        results = []\n",
        "        for pid in sorted_pids[:k]:\n",
        "            text = pid2text.get(pid, next((p[\"text\"] for p in passages_min if p[\"id\"] == pid), \"\"))\n",
        "            results.append((pid, text, float(score_map.get(pid, 0.0))))\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown retrieve mode: {mode}\")\n",
        "\n",
        "    # Apply metadata filters if requested (filter by pid only)\n",
        "    if any([min_date, max_date, allowed_sources, exclude_time_sensitive is not None]):\n",
        "        filtered_ids = filter_by_metadata([pid for pid,_,_ in results], min_date, max_date, allowed_sources, exclude_time_sensitive)\n",
        "        results = [(pid, pid2text.get(pid, \"\"), score) for pid,_,score in results if pid in filtered_ids]\n",
        "\n",
        "    return results\n",
        "\n",
        "# ---------- Quick sample test (safe) ----------\n",
        "q = eval_queries[0][\"query\"] if 'eval_queries' in globals() and len(eval_queries)>0 else \"کووڈ-19 کی عام علامات کیا ہیں؟\"\n",
        "print(\"Sample dense top-5 ids:\", [r[0] for r in dense_retrieve(q, k=5)])\n",
        "print(\"Sample bm25_new top-5 ids:\", [r[0] for r in bm25_new_retrieve(q, k=5)])\n",
        "print(\"Sample hybrid_score top-5 ids:\", [r[0] for r in retrieve(q, k=5, mode='hybrid_score')])\n",
        "print(\"Sample hybrid_rrf top-5 ids:\", [r[0] for r in retrieve(q, k=5, mode='hybrid_rrf')])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d4bb2873372d4af38ba08add7062d762",
            "ef5e87a6999f4c41aae94912ef9260c4",
            "fd9fbd8bf32741d6abaf07b40b4d8cce",
            "9714607204634ea7bbc1330c18292649",
            "7df0e28e26de4ecb896455aa338859ba",
            "1a40d2d1c66649699bde8f8d720e7ae3",
            "f7b9c18207c74ca1a8d57059c97820a6",
            "9b6c8498925948aab9eb15961166849e",
            "3fb6727bd71c4449bd851ba04b805fa5",
            "d61441c2297742938b96a4e77bf6c147",
            "ab5bb5578b344aee80b693419a1d2f0a",
            "877e8770dc04431ba4e7ce7c40175710",
            "39e33581d69f48c7ae10b26f3f08776e",
            "d5987955e4ca4e058dd65decc8d35421",
            "15361e6ff02c4207a8c8bbbcdcdd6295",
            "7243ab1fcd3f4d2080ccb4ea759ff505",
            "a3081a08f2a34428a8d5a710141bd9e5",
            "9ff3f6f409ea4ba39dd7cc6a19523044",
            "4f49e8cbc69b444f848e7cae2761958f",
            "4351a376029143ea9da4bf51fe09688f",
            "6b3589bc64514ecbad0e7508a3c3fed4",
            "4aa40708e1c34bc0b0284729a83374eb",
            "5ec4dfaeb5854b2197065270f4440dd6",
            "c7f3f31aa856470682a76eb0e04cd5ab",
            "9bb74f82ebb145119869f9e1370ed042",
            "60b9ff76dc2e4cc9896965571714ae59",
            "a1e2a57b69ae457c966282398f14ff0a",
            "ad6538379ff6429b8b536a1b9b2b5984",
            "a5e2e3bf3ffc409a9812956df32a8d45",
            "c0f9737d564c40feab5bb21a25cb9c48",
            "154e1d919ebd44639a0c654d3e3de354",
            "79fe4d5360314c379d71df52e6cdadca",
            "235d6726d0aa4e9d85e10a9e475e59ca",
            "8c17ea91b26f47e589bef7d0e91ec5a9",
            "2972bc84107b48dc8ab3bea489910112",
            "7c5fad704f7646dd9db49fcf9bf82bac",
            "b23fe60e3edf4a859441b32e86751ba0",
            "9fc3e73306d64c0b991352c7b7d3e576",
            "dae52c8e72ab483198daff08e9a0f478",
            "401805a96f2049e4acd9821002ed816a",
            "e20b92caf4524661b537320f1238656e",
            "f316dd9789004a50bc6f2b9895f3aa93",
            "7b5f734da22d487e9f9febd40248a218",
            "fc5942cc9593468d8e7130676fe9bb8e",
            "8aa6d614be294bb689f276ff23339fd8",
            "6d57dcf859b04a3faed28d12c95fab9f",
            "6bafe9e2a13b42df8e2e5fb4637463e3",
            "9f14349a685b46d3a7eff4350659d686",
            "36a7a4ed42ba40adb4ca9c90f9f7b9d2",
            "9c689d42619943b3b39da41a0f9359b3",
            "5cd1fbc6631e4e72b4b04fa99571aff9",
            "f82dcf67c4a946fb873dd5af80f433c2",
            "03693a7b801046a987b3bd6c2720439a",
            "c221412a3c8d49b09f3956f88520c15a",
            "ca1fc3abe1a24cb3a7b6f9a05946765d"
          ]
        },
        "id": "HpcqmjkddCM5",
        "outputId": "7b1020eb-d71a-45d8-c73e-e3d160fd71c2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating bm25 retriever:   0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d4bb2873372d4af38ba08add7062d762"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "bm25 retriever evaluation summary:\n",
            "  n_queries: 100\n",
            "  MRR: 0.880\n",
            "  Recall@1: 0.830\n",
            "  Recall@5: 0.950\n",
            "  Precision@1: 0.830\n",
            "  Precision@5: 0.216\n",
            "  latency_mean_s: 0.011\n",
            "  latency_median_s: 0.010\n",
            "  Total misses: 5 / 100. Showing up to 3 misses:\n",
            "   Query: کووڈ-19 ویکسین کا بنیادی مقصد کیا ہے؟\n",
            "    Positives: ['p0007']\n",
            "    Retrieved top ids: ['p0028', 'p0050', 'p0051', 'p0027', 'p0039']\n",
            "   Query: وینٹیلیشن وبا کے دوران کیوں اہم ہے؟\n",
            "    Positives: ['p0020']\n",
            "    Retrieved top ids: ['p0017', 'p0060', 'p0031', 'p0048', 'p0027']\n",
            "   Query: ویکسین سائیڈ ایفیکٹس کی نگرانی کیسے کی جاتی ہے؟\n",
            "    Positives: ['p0039']\n",
            "    Retrieved top ids: ['p0058', 'p0040', 'p0032', 'p0051', 'p0011']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating dense retriever:   0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "877e8770dc04431ba4e7ce7c40175710"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "dense retriever evaluation summary:\n",
            "  n_queries: 100\n",
            "  MRR: 0.856\n",
            "  Recall@1: 0.790\n",
            "  Recall@5: 0.930\n",
            "  Precision@1: 0.790\n",
            "  Precision@5: 0.210\n",
            "  latency_mean_s: 0.011\n",
            "  latency_median_s: 0.010\n",
            "  Total misses: 7 / 100. Showing up to 3 misses:\n",
            "   Query: کووڈ-19 کا بنیادی پھیلاؤ کا راستہ کیا ہے؟\n",
            "    Positives: ['p0028']\n",
            "    Retrieved top ids: ['p0025', 'p0059', 'p0038', 'p0010', 'p0033']\n",
            "   Query: ویکسین کی افادیت وقت کے ساتھ کیوں کم ہو سکتی ہے؟\n",
            "    Positives: ['p0032']\n",
            "    Retrieved top ids: ['p0008', 'p0052', 'p0007', 'p0025', 'p0059']\n",
            "   Query: ویکسین بوسٹر کب ضروری سمجھی جاتی ہے؟\n",
            "    Positives: ['p0011', 'p0032']\n",
            "    Retrieved top ids: ['p0051', 'p0052', 'p0033', 'p0008', 'p0007']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating hybrid_interleave retriever:   0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5ec4dfaeb5854b2197065270f4440dd6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "hybrid_interleave retriever evaluation summary:\n",
            "  n_queries: 100\n",
            "  MRR: 0.856\n",
            "  Recall@1: 0.790\n",
            "  Recall@5: 0.930\n",
            "  Precision@1: 0.790\n",
            "  Precision@5: 0.210\n",
            "  latency_mean_s: 0.011\n",
            "  latency_median_s: 0.010\n",
            "  Total misses: 7 / 100. Showing up to 3 misses:\n",
            "   Query: کووڈ-19 کا بنیادی پھیلاؤ کا راستہ کیا ہے؟\n",
            "    Positives: ['p0028']\n",
            "    Retrieved top ids: ['p0025', 'p0059', 'p0038', 'p0010', 'p0033']\n",
            "   Query: ویکسین کی افادیت وقت کے ساتھ کیوں کم ہو سکتی ہے؟\n",
            "    Positives: ['p0032']\n",
            "    Retrieved top ids: ['p0008', 'p0052', 'p0007', 'p0025', 'p0059']\n",
            "   Query: ویکسین بوسٹر کب ضروری سمجھی جاتی ہے؟\n",
            "    Positives: ['p0011', 'p0032']\n",
            "    Retrieved top ids: ['p0051', 'p0052', 'p0033', 'p0008', 'p0007']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating hybrid_score retriever:   0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8c17ea91b26f47e589bef7d0e91ec5a9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "hybrid_score retriever evaluation summary:\n",
            "  n_queries: 100\n",
            "  MRR: 0.936\n",
            "  Recall@1: 0.890\n",
            "  Recall@5: 0.990\n",
            "  Precision@1: 0.890\n",
            "  Precision@5: 0.230\n",
            "  latency_mean_s: 0.011\n",
            "  latency_median_s: 0.010\n",
            "  Total misses: 1 / 100. Showing up to 3 misses:\n",
            "   Query: کووڈ-19 کے خلاف طویل مدتی تیاری میں کون سی چیزیں شامل ہونی چاہئیں؟\n",
            "    Positives: ['p0060', 'p0052']\n",
            "    Retrieved top ids: ['p0021', 'p0025', 'p0044', 'p0059', 'p0046']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating hybrid_rrf retriever:   0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8aa6d614be294bb689f276ff23339fd8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "hybrid_rrf retriever evaluation summary:\n",
            "  n_queries: 100\n",
            "  MRR: 0.906\n",
            "  Recall@1: 0.840\n",
            "  Recall@5: 0.990\n",
            "  Precision@1: 0.840\n",
            "  Precision@5: 0.230\n",
            "  latency_mean_s: 0.011\n",
            "  latency_median_s: 0.010\n",
            "  Total misses: 1 / 100. Showing up to 3 misses:\n",
            "   Query: کووڈ-19 کے خلاف طویل مدتی تیاری میں کون سی چیزیں شامل ہونی چاہئیں؟\n",
            "    Positives: ['p0060', 'p0052']\n",
            "    Retrieved top ids: ['p0021', 'p0025', 'p0044', 'p0010', 'p0059']\n"
          ]
        }
      ],
      "source": [
        "# Cell 9a: Run after above cell 9. Cell 9 creates B2M5 + Dense hybrid and below\n",
        "# cell evaluates its performance:\n",
        "# Cell 9c: Validation diagnostics — Recall@1, Recall@5, MRR, Precision@k for retrievers\n",
        "# - Works with any mode supported by your Cell 9 wrapper: 'bm25', 'dense', 'hybrid_interleave', 'hybrid_score', 'hybrid_rrf'\n",
        "# - Calls retrieve(...) and computes retrieval metrics\n",
        "# - Outputs summary metrics and a few examples of misses\n",
        "\n",
        "import time, statistics\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "DEFAULT_K = 5\n",
        "RECALL_KS = [1, 5]\n",
        "PRECISION_KS = [1, 5]\n",
        "\n",
        "def evaluate_retriever(eval_items, mode=\"hybrid_score\", k=DEFAULT_K,\n",
        "                       recall_ks=RECALL_KS, precision_ks=PRECISION_KS):\n",
        "    per_query = []\n",
        "    latencies = []\n",
        "    rr_list = []\n",
        "    recall_counts = {rk: 0 for rk in recall_ks}\n",
        "    precision_sums = {pk: 0.0 for pk in precision_ks}\n",
        "    total = 0\n",
        "\n",
        "    for item in tqdm(eval_items, desc=f\"Evaluating {mode} retriever\"):\n",
        "        total += 1\n",
        "        q = item.get(\"query\") or item.get(\"question\") or item.get(\"q\") or \"\"\n",
        "        positive_ids = item.get(\"positive_ids\") or item.get(\"positive_id\") or []\n",
        "        if isinstance(positive_ids, str):\n",
        "            positive_ids = [positive_ids]\n",
        "        positive_ids = [str(x) for x in positive_ids]\n",
        "\n",
        "        t0 = time.time()\n",
        "        try:\n",
        "            hits = retrieve(q, k=k, mode=mode)   # (pid, text, score)\n",
        "        except Exception as e:\n",
        "            hits = []\n",
        "            print(f\"[eval] retrieve error for query {q[:60]}... -> {e}\")\n",
        "        latency = time.time() - t0\n",
        "        latencies.append(latency)\n",
        "\n",
        "        retrieved_ids = [r[0] for r in hits]\n",
        "\n",
        "        # Reciprocal rank: first position among positives\n",
        "        rr = 0.0\n",
        "        for rank, pid in enumerate(retrieved_ids, start=1):\n",
        "            if pid in positive_ids:\n",
        "                rr = 1.0 / rank\n",
        "                break\n",
        "        rr_list.append(rr)\n",
        "\n",
        "        # Recall@k and Precision@k\n",
        "        for rk in recall_ks:\n",
        "            recall_counts[rk] += 1 if any(pid in positive_ids for pid in retrieved_ids[:rk]) else 0\n",
        "        for pk in precision_ks:\n",
        "            num_pos_in_topk = sum(1 for pid in retrieved_ids[:pk] if pid in positive_ids)\n",
        "            precision_sums[pk] += (num_pos_in_topk / pk)\n",
        "\n",
        "        per_query.append({\n",
        "            \"query\": q,\n",
        "            \"positive_ids\": positive_ids,\n",
        "            \"retrieved_ids\": retrieved_ids,\n",
        "            \"reciprocal_rank\": rr,\n",
        "            \"latency\": latency\n",
        "        })\n",
        "\n",
        "    n = total if total else 1\n",
        "    mrr = sum(rr_list) / n\n",
        "    recall_at = {rk: recall_counts[rk] / n for rk in recall_ks}\n",
        "    precision_at = {pk: precision_sums[pk] / n for pk in precision_ks}\n",
        "    latency_mean = statistics.mean(latencies) if latencies else 0.0\n",
        "    latency_median = statistics.median(latencies) if latencies else 0.0\n",
        "\n",
        "    summary = {\n",
        "        \"n_queries\": n,\n",
        "        \"MRR\": mrr,\n",
        "        **{f\"Recall@{rk}\": recall_at[rk] for rk in recall_ks},\n",
        "        **{f\"Precision@{pk}\": precision_at[pk] for pk in precision_ks},\n",
        "        \"latency_mean_s\": latency_mean,\n",
        "        \"latency_median_s\": latency_median\n",
        "    }\n",
        "\n",
        "    return summary, per_query\n",
        "\n",
        "# ---------- Run evaluation ----------\n",
        "# Use eval_queries_val if defined, else fall back to eval_queries\n",
        "eval_items = eval_queries_val if 'eval_queries_val' in globals() else eval_queries\n",
        "\n",
        "# Evaluate all retriever modes\n",
        "modes = [\"bm25\", \"dense\", \"hybrid_interleave\", \"hybrid_score\", \"hybrid_rrf\"]\n",
        "results = {}\n",
        "for m in modes:\n",
        "    summary, records = evaluate_retriever(eval_items, mode=m, k=DEFAULT_K)\n",
        "    results[m] = summary\n",
        "    print(f\"\\n{m} retriever evaluation summary:\")\n",
        "    for k,v in summary.items():\n",
        "        print(f\"  {k}: {v:.3f}\" if isinstance(v,float) else f\"  {k}: {v}\")\n",
        "\n",
        "    # Show a few misses\n",
        "    misses = [r for r in records if r[\"reciprocal_rank\"] == 0.0]\n",
        "    print(f\"  Total misses: {len(misses)} / {len(records)}. Showing up to 3 misses:\")\n",
        "    for r in misses[:3]:\n",
        "        print(\"   Query:\", r[\"query\"][:80])\n",
        "        print(\"    Positives:\", r[\"positive_ids\"])\n",
        "        print(\"    Retrieved top ids:\", r[\"retrieved_ids\"][:8])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 775
        },
        "collapsed": true,
        "id": "oVJPa0YidJQ2",
        "outputId": "60f1cfb3-f010-47d9-b3d1-0c2a7e907fb3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name sentence-transformers/fine_tuned_sbert_urdu. Creating a new one with mean pooling.\n"
          ]
        },
        {
          "ename": "OSError",
          "evalue": "sentence-transformers/fine_tuned_sbert_urdu is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/sentence-transformers/fine_tuned_sbert_urdu/resolve/main/adapter_config.json",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;31m# This is slightly better for only 1 file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m             hf_hub_download(\n\u001b[0m\u001b[1;32m    480\u001b[0m                 \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m   1008\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;31m# Otherwise, raise appropriate error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m         \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1654\u001b[0m         \u001b[0;31m# Unauthorized => likely a token issue => let's raise the actual error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1655\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mhead_call_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1656\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1542\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1543\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1544\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers, endpoint)\u001b[0m\n\u001b[1;32m   1459\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1460\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1461\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    284\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_backoff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    451\u001b[0m             )\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRepositoryNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-69370fc9-2740045d06f735b1640baa99;15d4412f-53d9-4943-ba12-e8c480bf0276)\n\nRepository Not Found for url: https://huggingface.co/sentence-transformers/fine_tuned_sbert_urdu/resolve/main/adapter_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\nInvalid username or password.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1512332818.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Optional Cell:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0membedder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fine_tuned_sbert_urdu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[0m\n\u001b[1;32m    337\u001b[0m                 )\n\u001b[1;32m    338\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m                 modules = self._load_auto_model(\n\u001b[0m\u001b[1;32m    340\u001b[0m                     \u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                     \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36m_load_auto_model\u001b[0;34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs, has_modules)\u001b[0m\n\u001b[1;32m   2110\u001b[0m         \u001b[0mconfig_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshared_kwargs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconfig_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mshared_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mconfig_kwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2112\u001b[0;31m         transformer_model = Transformer(\n\u001b[0m\u001b[1;32m   2113\u001b[0m             \u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m             \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_folder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/models/Transformer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path, backend)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mconfig_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_peft_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_peft_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/models/Transformer.py\u001b[0m in \u001b[0;36m_load_config\u001b[0;34m(self, model_name_or_path, cache_dir, backend, config_args)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \"\"\"\n\u001b[1;32m    137\u001b[0m         if (\n\u001b[0;32m--> 138\u001b[0;31m             find_adapter_config_file(\n\u001b[0m\u001b[1;32m    139\u001b[0m                 \u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/peft_utils.py\u001b[0m in \u001b[0;36mfind_adapter_config_file\u001b[0;34m(model_id, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, _commit_hash)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0madapter_cached_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mADAPTER_CONFIG_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         adapter_cached_filename = cached_file(\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mADAPTER_CONFIG_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m     \"\"\"\n\u001b[0;32m--> 322\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;31m# We cannot recover from them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGatedRepoError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m             raise OSError(\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0;34mf\"{path_or_repo_id} is not a local folder and is not a valid model identifier \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;34m\"listed on 'https://huggingface.co/models'\\nIf this is a private repository, make sure to pass a token \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: sentence-transformers/fine_tuned_sbert_urdu is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`"
          ]
        }
      ],
      "source": [
        "# Optional Cell:\n",
        "from sentence_transformers import SentenceTransformer\n",
        "embedder = SentenceTransformer(\"fine_tuned_sbert_urdu\").to(\"cuda\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bn2TmMs8dNux"
      },
      "outputs": [],
      "source": [
        "# # Cell 10a: Install Libraries for QLoRA Fine-Tuning\n",
        "!pip install -q --upgrade transformers bitsandbytes accelerate peft trl\n",
        "!pip install -q --upgrade sentence-transformers faiss-cpu datasets sacrebleu\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\" # Disable external logging to save speed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mEfC9M3NLUg"
      },
      "outputs": [],
      "source": [
        "# # Crash Session Code:\n",
        "# # Cell 10_Fix: The Nuclear Library Fix\n",
        "# import os\n",
        "\n",
        "# print(\"🛑 Uninstalling old libraries...\")\n",
        "# !pip uninstall -y transformers bitsandbytes accelerate\n",
        "\n",
        "# print(\"⬇️ Installing fresh, compatible versions...\")\n",
        "# !pip install -q -U bitsandbytes\n",
        "# !pip install -q -U accelerate\n",
        "# !pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "# !pip install -q -U datasets\n",
        "\n",
        "# print(\"✅ Installation Done.\")\n",
        "# print(\"💥 KILLING RUNTIME TO FORCE RELOAD... (Expect a 'Session Crashed' message!)\")\n",
        "\n",
        "# # This command kills the Python process immediately\n",
        "# os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ny1U-ecUSm_g"
      },
      "outputs": [],
      "source": [
        "# # Cell 10_Update: Force Install from Source (The \"Bleeding Edge\" Fix)\n",
        "# import os\n",
        "\n",
        "# print(\"🛑 Uninstalling all potentially conflicting libraries...\")\n",
        "# !pip uninstall -y transformers tokenizers bitsandbytes accelerate\n",
        "\n",
        "# print(\"⬇️ Installing 'Bleeding Edge' Transformers from GitHub...\")\n",
        "# # We install directly from source to get the Qwen 2.5 fixes\n",
        "# !pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "# !pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "# !pip install -q -U bitsandbytes\n",
        "# !pip install -q -U datasets\n",
        "\n",
        "# print(\"✅ Installation Complete.\")\n",
        "# print(\"⚠️ CRITICAL STEP: Go to 'Runtime' -> 'Restart Session' NOW.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zg28mnicSFg6"
      },
      "outputs": [],
      "source": [
        "# # Cell 10_Final_Golden: Conflict-Free Fine-Tuning\n",
        "# import os\n",
        "# import torch\n",
        "# from datasets import Dataset\n",
        "# import json\n",
        "# from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
        "# from peft import LoraConfig, prepare_model_for_kbit_training\n",
        "# from trl import SFTTrainer\n",
        "\n",
        "# # Optimizations\n",
        "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "# torch.cuda.empty_cache()\n",
        "\n",
        "# # 1. Load Tokenizer & Model\n",
        "# model_id = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "\n",
        "# bnb_config = BitsAndBytesConfig(\n",
        "#     load_in_4bit=True,\n",
        "#     bnb_4bit_quant_type=\"nf4\",\n",
        "#     bnb_4bit_compute_dtype=torch.float16,\n",
        "#     bnb_4bit_use_double_quant=True,\n",
        "# )\n",
        "\n",
        "# print(f\"⏳ Loading {model_id}...\")\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "# tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# model = AutoModelForCausalLM.from_pretrained(\n",
        "#     model_id,\n",
        "#     quantization_config=bnb_config,\n",
        "#     device_map=\"auto\",\n",
        "#     trust_remote_code=True\n",
        "# )\n",
        "\n",
        "# # 2. Prepare for Training\n",
        "# model.gradient_checkpointing_enable()\n",
        "# model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# # 3. Prepare Data\n",
        "# def format_instruction(sample):\n",
        "#     return {\n",
        "#         \"messages\": [\n",
        "#             {\"role\": \"system\", \"content\": \"آپ ایک ماہر ڈاکٹر ہیں۔ صارف کے سوال کا اردو میں درست اور مختصر جواب دیں۔\"},\n",
        "#             {\"role\": \"user\", \"content\": sample['query']},\n",
        "#             {\"role\": \"assistant\", \"content\": sample.get('gold_answer', sample.get('answer', ''))}\n",
        "#         ]\n",
        "#     }\n",
        "\n",
        "# data = []\n",
        "# try:\n",
        "#     with open(\"eval_queries.jsonl\", \"r\") as f:\n",
        "#         for line in f:\n",
        "#             data.append(json.loads(line))\n",
        "# except FileNotFoundError:\n",
        "#     print(\"⚠️ Error: eval_queries.jsonl not found.\")\n",
        "\n",
        "# train_data = data[:int(0.9 * len(data))]\n",
        "# hf_dataset = Dataset.from_list(train_data)\n",
        "\n",
        "# def process_data(row):\n",
        "#     # We create a column named \"text\" explicitly\n",
        "#     return {\"text\": tokenizer.apply_chat_template(format_instruction(row)[\"messages\"], tokenize=False)}\n",
        "\n",
        "# dataset = hf_dataset.map(process_data)\n",
        "# print(f\"✅ Data Prepared: {len(dataset)} examples. Column names: {dataset.column_names}\")\n",
        "\n",
        "# # 4. LoRA Config\n",
        "# peft_config = LoraConfig(\n",
        "#     r=16, lora_alpha=16, lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\",\n",
        "#     target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "# )\n",
        "\n",
        "# # 5. Training Args\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir=\"./qwen_urdu_finetuned\",\n",
        "#     per_device_train_batch_size=1,\n",
        "#     gradient_accumulation_steps=4,\n",
        "#     learning_rate=2e-4,\n",
        "#     logging_steps=5,\n",
        "#     fp16=True,\n",
        "#     max_steps=30,\n",
        "#     save_strategy=\"no\",\n",
        "#     optim=\"paged_adamw_32bit\",\n",
        "#     report_to=\"none\"\n",
        "# )\n",
        "\n",
        "# # 6. Trainer (Argument Removed!)\n",
        "# trainer = SFTTrainer(\n",
        "#     model=model,\n",
        "#     train_dataset=dataset,\n",
        "#     peft_config=peft_config,\n",
        "#     # dataset_text_field=\"text\",  <-- REMOVED THIS LINE\n",
        "#     max_seq_length=256,\n",
        "#     args=training_args,\n",
        "#     tokenizer=tokenizer\n",
        "# )\n",
        "\n",
        "# print(\"🚀 Starting Fine-Tuning...\")\n",
        "# trainer.train()\n",
        "# print(\"✅ Fine-Tuning Complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639,
          "referenced_widgets": [
            "838729c55f994afdb687b026d3b928db",
            "e77d07f4dc2a4a03b2d07cfbed0452c2",
            "1ceba487e19049f6926954e0fdda2737",
            "d7ee6af493a54bba8005b10ece48c894",
            "5630675d7e6e4a78bca1430fbd6f6840",
            "e42f499bad4148358e70e6cdacc2efed",
            "a3be0dafde0a4073832003dd934d0484",
            "029938f2088b40d4a4ce151ad9dee14e",
            "d2b00d0e636946c3af126fe9f7e0b923",
            "87f710eac1ff41468fe888e9b71147ee",
            "c3ae708260e04b6099f1a3a210821cb3",
            "81567a486a4f4fc683ed07399a82cf45",
            "8c05ef7974ec4c249978fc9ac44f90da",
            "e20521b720e94e8db29cb151e1ea521b",
            "4310f3537d8f48a28d272e66b84f017e",
            "28c1ceae68224658876e818048d8dc84",
            "5d7006e46a9740708bfa9a64e9163391",
            "ded57d641dc9449c947db7f31e3e8d6f",
            "187f20cc1b2d4b959c0d0c88b6600e44",
            "59e685d6d6cd4e299fb963c95d0545cd",
            "1a57217e778246e2871a0ad2c0a827e4",
            "d2b19ec2451b46ceaa1278242f459d5c"
          ]
        },
        "id": "Kz6CAMmiGR1a",
        "outputId": "e08fe2ec-bce0-4de6-d546-9704a955abb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.2)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.9.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "838729c55f994afdb687b026d3b928db"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Base Model Loaded.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/90 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "81567a486a4f4fc683ed07399a82cf45"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Data Prepared: 90 training examples.\n"
          ]
        }
      ],
      "source": [
        "# Cell 10b: Load Base Qwen Model & Prepare Training Data\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from datasets import Dataset\n",
        "import json\n",
        "\n",
        "# Add this line to ensure bitsandbytes is up-to-date\n",
        "!pip install -U bitsandbytes\n",
        "\n",
        "# 1. Load Tokenizer & Base Model (4-bit)\n",
        "model_id = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token # Fix for training\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "print(\"✅ Base Model Loaded.\")\n",
        "\n",
        "# 2. Prepare Data from 'synthetic_qa_pairs.jsonl'\n",
        "# We format it into the \"Chat\" format that Qwen expects\n",
        "def format_instruction(sample):\n",
        "    # We simulate a \"Closed Book\" question (Independent Generator)\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"آپ ایک ماہر ڈاکٹر ہیں۔ صارف کے سوال کا اردو میں درست اور مختصر جواب دیں۔\"},\n",
        "            {\"role\": \"user\", \"content\": sample['query']},\n",
        "            {\"role\": \"assistant\", \"content\": sample.get('gold_answer', sample.get('answer', ''))} # Fallback if key differs\n",
        "        ]\n",
        "    }\n",
        "\n",
        "# Load Synthetic Pairs (File 5) for Training\n",
        "data = []\n",
        "# NOTE: Ensure synthetic_qa_pairs.jsonl has an 'answer' or 'gold_answer' field.\n",
        "# If your synthetic file ONLY has queries, we must use the corpus to find answers or use a subset of eval data (excluding validation).\n",
        "# For now, let's assume we use a split of eval_queries for training to demonstrate the code.\n",
        "with open(\"eval_queries.jsonl\", \"r\") as f: # Using eval queries for demo (in real research, use separate train set)\n",
        "    for line in f:\n",
        "        data.append(json.loads(line))\n",
        "\n",
        "# Split: 90% Train, 10% Test\n",
        "split_idx = int(0.9 * len(data))\n",
        "train_data = data[:split_idx]\n",
        "val_data = data[split_idx:]\n",
        "\n",
        "hf_dataset = Dataset.from_list(train_data)\n",
        "# Apply Chat Template\n",
        "def process_data(row):\n",
        "    formatted = tokenizer.apply_chat_template(format_instruction(row)[\"messages\"], tokenize=False)\n",
        "    return {\"text\": formatted}\n",
        "\n",
        "dataset = hf_dataset.map(process_data)\n",
        "print(f\"✅ Data Prepared: {len(dataset)} training examples.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "391WGL1yfCL-"
      },
      "outputs": [],
      "source": [
        "# # Cell 10c: Fine-Tune the Generator (QLoRA) - FIXED\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# 1. Prepare Model for Training (Gradient Checkpointing)\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# 2. LoRA Configuration\n",
        "peft_config = LoraConfig(\n",
        "    r=16,           # Rank\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        ")\n",
        "\n",
        "# 3. Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./qwen_urdu_finetuned\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    logging_steps=5,\n",
        "    fp16=True,\n",
        "    max_steps=30,  # Keeping it short for a quick success test\n",
        "    save_strategy=\"no\",\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    report_to=\"none\" # Disable WandB explicitly\n",
        ")\n",
        "\n",
        "# 4. Trainer (Pass peft_config here!)\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,      # <--- The Trainer handles the wrapping now\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=512,\n",
        "    args=training_args,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "print(\"🚀 Starting Generator Fine-Tuning...\")\n",
        "trainer.train()\n",
        "print(\"✅ Fine-Tuning Complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njFgyLIJMF97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "39388595896f4e9f90cc0928ec3889dd",
            "636ec355626b4ff892b64b050eb13c3b",
            "e33a7444cccd45b7ac0f432ab8527ba6",
            "397c5dd833ac4abf99ef4c496f4ffcd4",
            "69969929f51c46d3a2296939550275cc",
            "97527694507c4b71b5c3421940f84db8",
            "16ae194da3684de1a5f7c45d1a5e9b0b",
            "82657e668404473c87d93bb123440616",
            "61715fec96b74643b25ef65552843794",
            "576d4d6b7bf04f95a8a3641591426e27",
            "b569bac6ae0b4cb58682f5fa7a1850be"
          ]
        },
        "outputId": "74fe3305-37d5-495e-fd84-3e8b6001078c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running Evaluation on Validation Set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "39388595896f4e9f90cc0928ec3889dd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Independent Generator Stats ===\n",
            "BLEU: 1.81\n",
            "chrF: 22.80\n"
          ]
        }
      ],
      "source": [
        "# # Cell 10d: Evaluate Independent Generator (Accuracy Check)\n",
        "from tqdm.auto import tqdm\n",
        "import sacrebleu\n",
        "\n",
        "def generate_answer(query):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"آپ ایک ماہر ڈاکٹر ہیں۔ مختصر جواب دیں۔\"},\n",
        "        {\"role\": \"user\", \"content\": query}\n",
        "    ]\n",
        "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer([text], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(**inputs, max_new_tokens=128, temperature=0.1)\n",
        "\n",
        "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "    # Extract only the assistant's part\n",
        "    if \"assistant\\n\" in response:\n",
        "        response = response.split(\"assistant\\n\")[-1]\n",
        "    return response.strip()\n",
        "\n",
        "print(\"running Evaluation on Validation Set...\")\n",
        "preds, refs = [], []\n",
        "\n",
        "# Evaluate on the 10% validation split we made in 10b\n",
        "for item in tqdm(val_data):\n",
        "    pred = generate_answer(item['query'])\n",
        "    preds.append(pred)\n",
        "    refs.append([item['gold_answer']])\n",
        "\n",
        "# Metrics\n",
        "bleu = sacrebleu.corpus_bleu(preds, refs)\n",
        "chrf = sacrebleu.corpus_chrf(preds, refs)\n",
        "\n",
        "print(\"\\n=== Independent Generator Stats ===\")\n",
        "print(f\"BLEU: {bleu.score:.2f}\")\n",
        "print(f\"chrF: {chrf.score:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUpsqEBsMJ8z"
      },
      "outputs": [],
      "source": [
        "# # Cell 10e: Save Fine-Tuned Adapter to Drive\n",
        "SAVE_PATH = \"/content/drive/MyDrive/models/qwen_urdu_adapter\"\n",
        "\n",
        "print(f\"💾 Saving adapter to {SAVE_PATH}...\")\n",
        "model.save_pretrained(SAVE_PATH)\n",
        "tokenizer.save_pretrained(SAVE_PATH)\n",
        "print(\"✅ Adapter Saved! You can skip training next time.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rr1ihR5rMQic"
      },
      "outputs": [],
      "source": [
        "# # Cell 10f: Fast Load (Base Model + Saved Adapter)\n",
        "# # Run this INSTEAD of 10b, 10c, 10d, 10e in future sessions\n",
        "# import torch\n",
        "# from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "# from peft import PeftModel\n",
        "\n",
        "# ADAPTER_PATH = \"/content/drive/MyDrive/models/qwen_urdu_adapter\"\n",
        "# BASE_MODEL_ID = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "\n",
        "# # 1. Load Base\n",
        "# print(\"⏳ Loading Base Model...\")\n",
        "# bnb_config = BitsAndBytesConfig(\n",
        "#     load_in_4bit=True,\n",
        "#     bnb_4bit_compute_dtype=torch.float16,\n",
        "#     bnb_4bit_quant_type=\"nf4\"\n",
        "# )\n",
        "# base_model = AutoModelForCausalLM.from_pretrained(\n",
        "#     BASE_MODEL_ID, quantization_config=bnb_config, device_map=\"auto\"\n",
        "# )\n",
        "# tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n",
        "\n",
        "# # 2. Load Adapter\n",
        "# print(\"🔗 Attaching Fine-Tuned Adapter...\")\n",
        "# model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)\n",
        "\n",
        "# print(\"✅ Fine-Tuned Generator Ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmMpp0rzXG-Q",
        "outputId": "d99bb3a0-2ab3-4092-a649-cb153a5c7f14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⬇️ Installing clean libraries for Qwen...\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m520.9/520.9 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 5.1.2 requires transformers<5.0.0,>=4.41.0, but you have transformers 5.0.0.dev0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m✅ Libraries installed. PLEASE RESTART SESSION ONE LAST TIME (Runtime -> Restart Session).\n"
          ]
        }
      ],
      "source": [
        "# Cell 10_Install: Clean Install for Qwen Inference\n",
        "import os\n",
        "\n",
        "print(\"⬇️ Installing clean libraries for Qwen...\")\n",
        "# We install specific versions known to work together\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U accelerate\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "\n",
        "print(\"✅ Libraries installed. PLEASE RESTART SESSION ONE LAST TIME (Runtime -> Restart Session).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsdGvSiQbz_T",
        "outputId": "c3ff246a-b583-42ee-af77-12cb22657c38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚑 Repairing libraries...\n",
            "Found existing installation: transformers 4.57.3\n",
            "Uninstalling transformers-4.57.3:\n",
            "  Successfully uninstalled transformers-4.57.3\n",
            "Found existing installation: tokenizers 0.22.1\n",
            "Uninstalling tokenizers-0.22.1:\n",
            "  Successfully uninstalled tokenizers-0.22.1\n",
            "Found existing installation: bitsandbytes 0.48.2\n",
            "Uninstalling bitsandbytes-0.48.2:\n",
            "  Successfully uninstalled bitsandbytes-0.48.2\n",
            "Found existing installation: accelerate 1.12.0\n",
            "Uninstalling accelerate-1.12.0:\n",
            "  Successfully uninstalled accelerate-1.12.0\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "peft 0.18.0 requires accelerate>=0.21.0, which is not installed.\n",
            "sentence-transformers 5.1.2 requires transformers<5.0.0,>=4.41.0, but you have transformers 5.0.0.dev0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for accelerate (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.6/511.6 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h✅ Repair Complete. PLEASE RESTART SESSION (Runtime -> Restart Session).\n"
          ]
        }
      ],
      "source": [
        "# Cell 1_Repair: Fix Accidental Downgrade\n",
        "import os\n",
        "\n",
        "print(\"🚑 Repairing libraries...\")\n",
        "!pip uninstall -y transformers tokenizers bitsandbytes accelerate\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U datasets\n",
        "\n",
        "print(\"✅ Repair Complete. PLEASE RESTART SESSION (Runtime -> Restart Session).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563,
          "referenced_widgets": [
            "b8f0594be82c48448a8004e5aca2f09c",
            "ceb7221e53144425aab2882edc8fb600",
            "c7e08d3e9979427e85919b9334be5627",
            "96b65aa5867a4fa09718e86b05ded43b",
            "e71b188d0a974a938856c61c70481389",
            "4d9b4fc876e448958d518291756efff2",
            "8fe1e04e8e0549c3bab16ebf071f26be",
            "490341c19b584deebdd193ab24297445",
            "1f884ede6dca48c0a56d50629807bfbd",
            "409d7a1a7ac840c3bf83c30d0f34b8b2",
            "85b3702fa5eb402bb42dd439bed28e95",
            "da389b350337433781efcffb46bb8681",
            "1297297a72004a538e6c12d316299c25",
            "f27ead5fca444126ab0e9cda80718900",
            "e2b6834a9ca54b3cbebd29efcaf6705e",
            "09066b884c6146f89dfa929f33de3748",
            "43f6708d06e84e9497187ff7ff4b05ce",
            "bdf8cd7c73ae4cf99aab36b7830495de",
            "2e70209e0b4e47df81bd28e8c3fc978d",
            "90592800ff8541dc9bd0a4e027e52e2c",
            "31d926946f2b4b11a613b3ba8690dbb5",
            "c846cc2104d24d6f9e54a564bc1012e0",
            "19adbec50b364569a5b803ac226181c6",
            "d1aa82f909e6406f9cd238c5c11cbd8b",
            "1f9f469c6d4947589307bf54fb374db8",
            "f258d47fc5de4daf8c20079e669f53d7",
            "07882386801547bc84265a2af644ddc3",
            "17b928c1d9e541899a8479a6d3ae2fac",
            "6392d49f023e480b8a959c8516ce8df1",
            "4b45807daebb42b5b547f6d9fd9bff20",
            "12675aefec4a42fa9af46671d2efb6b5",
            "68534fa1d4364d37bae45ffbf6359a08",
            "6be7458957c04dfd80bb49ac14343241",
            "42e659bbb16c412691139bfede8f1d6e",
            "2e3908f9c3da4db1ad11b2ed8d91137c",
            "c7b9a5181db44c4b96c6ab0ec18893c9",
            "b0e421b44770409c854789dca593b0c2",
            "088a74bb1b564faaa9cf54526ee54f55",
            "83ff2879e3844188a6ab5d970a57d21e",
            "cdd39fb96a6147baafa8e1cf9795d5b1",
            "306d73abcf4a4069b66bbba5a001d837",
            "ff212976c3ea430eabfe00784ff19cb1",
            "9015d045216d4fbb95fb7364800fa1ce",
            "8aeee6e3d77e450ca3f59efca8733f10",
            "69db8f3e63db4e3d94b93daa8b1c0085",
            "bac23a14503d415a9cc9083b9a7d55b0",
            "0a5f1dac5bee42e6ad2819f2e5f38700",
            "5d992c26448b44eda730649ee190b160",
            "2bdf42960d554d1892d616433a5e02c1",
            "9fb85da185c04a9aa4ee364f7d8e53df",
            "f653157c90644576afb3cc8aee8a2bf6",
            "31ced63610624233b77616f44695fbab",
            "c39d6178de314a148a276f2ba5b1ab91",
            "4201bbde0e1949c5b6d46a638bbc0ac1",
            "9216fcedda614e9e99ce02fc61e90d37",
            "bdef189452d44b7391ef5096051e505c",
            "9bf50965a6344a0583b5e0bb839c7f96",
            "4498758d7b9d4e3a975fb3dc0d6fa2f9",
            "651585397a0a43b78aa7a26b58e5bc26",
            "57d656317c3a45d3b2809e4534bc33ee",
            "6ba27529c54140c9880360c9a923e1ad",
            "a403d2c04ffd418490c6a4cda7135bcf",
            "67e265b455b3466cb1fab5bcc43b67c5",
            "c7dffde81763450f807d42f8d56507c6",
            "72e9b9f598dd402fb565c9754890769c",
            "7283dc8450db4f809958e8748c612d14",
            "7a23a6bd5e164c8eaec5d356cf75f084",
            "295579626fa742cb9d416c88344cdf30",
            "a2826e2bbbc1462b884e8a208bc52828",
            "664e28d4960e41ad8455ca5d296c02db",
            "db391361293841b08542b7754a88eca4",
            "818ff8735c67476fa47cb1a950435129",
            "8748b54f1d2544bf9e1778fe6166f5c7",
            "e2918935c7894e468972b776d06f3817",
            "9cb8922b3f364eb483f8dc2e69d25df7",
            "09e9efac668947e381a312f659db0a57",
            "329514a6f31b4fd2aa32d9d98be2a784",
            "643d9625309044f0b3a068a1a9b147df",
            "14eb36a8c8194a49b6744917c2d7bc97",
            "e990482878b140a98541133402ecab62",
            "04b572fa1fac41d485ca29133b052674",
            "852c5aa63713414d853ecabeabb728a7",
            "704140461521446fa3697a96e21a9e71",
            "98942a5eff174fd5b1fe0c021713207f",
            "b862afd4d4514b608ffc95256e51a0ff",
            "f78a5157274543d18a018a32373aa116",
            "29b48a758ed94ce38992f49156d51de1",
            "0443706b6e574498894aa3f31d0699de",
            "dc275d0adab244418f0222e493bcd369",
            "126f7b57bfbc42ebb60aebc4c2e1aac4",
            "98f6372819cd4ea4b0329eccc5a3fe5d",
            "e95885a7110c4926897ba6a6e21f5f30",
            "3eb36a1eae7d41bcb330cf29e9626dda",
            "1c3b3b9845164b61a7050dee3e770f71",
            "bf5db3bd1fdd4663b715c47346adfdb7",
            "bb7d8f88587c4ea49fd0c18f2732aa8e",
            "6275d07c97644d25b081db6db3b8d0c5",
            "78d983eef7954453862b06fa55646f32",
            "7a43b31faf734b22aa8590b1bda13dd7",
            "e3590997d31f47cc9a53eaf961a5ed88",
            "a9f6dd1fc4f64714a635df8c24478ea9",
            "ad229f3e72304b13925863e39de266fb",
            "1f86eb2519d048ba8fdce1e2b26d4dd6",
            "7785554e63df491fa97dfd51b6b6e290",
            "57a02405ae3c444994af54b3794b799c",
            "e4e5d3f298c24c5b8fb7cfe616748078",
            "98073d161f434e30a3f6296fdc1176d8",
            "37b5d700445646b08dfacdbadaa605b4",
            "3e61e40a31124003b832a39fac264798",
            "cf45a6fc2e2143db98a278fa4073ec17",
            "54da7344c7d74fd784014bdac70047a5",
            "cd70c004330b47799217e1dff9a69edf",
            "d1fba92c46a44b759213c4d9c04ab862",
            "07c436725986452fb97da9b27c8d46ea",
            "fde317e8e8ad48f9a4704cb641da28f2",
            "4e04f983838f407aa9aa75e96eb4a3fe",
            "ee0d09a20c5141fb95b10a03ea09a33c",
            "edaaf861897e444d842b6da59ae13832",
            "9ddc42267ec14bad8ef034c6adf784be",
            "92b1cd7d90e94ad7b9145401b4a94a50",
            "e6dd70014d9a444d89cc02bd3fc908b0"
          ]
        },
        "id": "cp9eM8V4alh6",
        "outputId": "7fe66fb0-a4b0-4e04-90bf-de3a7836c6b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📂 Looking for files in /content...\n",
            "✅ Loaded 60 passages and 100 queries.\n",
            "Building Dense Index...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b8f0594be82c48448a8004e5aca2f09c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "da389b350337433781efcffb46bb8681",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "19adbec50b364569a5b803ac226181c6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "42e659bbb16c412691139bfede8f1d6e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "69db8f3e63db4e3d94b93daa8b1c0085",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bdef189452d44b7391ef5096051e505c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7a23a6bd5e164c8eaec5d356cf75f084",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "643d9625309044f0b3a068a1a9b147df",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dc275d0adab244418f0222e493bcd369",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e3590997d31f47cc9a53eaf961a5ed88",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "54da7344c7d74fd784014bdac70047a5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ COMPLETE: Data & Retriever Restored!\n"
          ]
        }
      ],
      "source": [
        "# Cell 1-9_Restore: Safe Data & Retriever Restoration (Fixed Path)\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "# 1. Install ONLY the missing retriever libraries (Safe Mode)\n",
        "!pip install -q sentence-transformers rank_bm25 faiss-cpu\n",
        "\n",
        "# 2. Load Data (Fixed Path for Local Content)\n",
        "# We look directly in /content/ or /content/data/\n",
        "base_path = Path(\"/content\")\n",
        "if (base_path / \"data\").exists():\n",
        "    DATA_DIR = base_path / \"data\"\n",
        "else:\n",
        "    DATA_DIR = base_path\n",
        "\n",
        "print(f\"📂 Looking for files in {DATA_DIR}...\")\n",
        "\n",
        "def load_jsonl(path):\n",
        "    data = []\n",
        "    try:\n",
        "        with open(path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                if line.strip():\n",
        "                    data.append(json.loads(line))\n",
        "        return data\n",
        "    except FileNotFoundError:\n",
        "        print(f\"❌ File not found: {path}\")\n",
        "        return []\n",
        "\n",
        "# Load the essential files\n",
        "passages_min = load_jsonl(DATA_DIR / \"urdu_covid_passages_min.jsonl\")\n",
        "eval_queries = load_jsonl(DATA_DIR / \"eval_queries.jsonl\")\n",
        "\n",
        "# STOP if data is missing\n",
        "if len(passages_min) == 0:\n",
        "    raise ValueError(\"⚠️ CRITICAL: passages_min is empty! Please upload 'urdu_covid_passages_min.jsonl' to the Files folder on the left.\")\n",
        "\n",
        "print(f\"✅ Loaded {len(passages_min)} passages and {len(eval_queries)} queries.\")\n",
        "\n",
        "# 3. Build BM25 Index\n",
        "from rank_bm25 import BM25Okapi\n",
        "import re\n",
        "def normalize(text): return re.sub(r\"\\s+\", \" \", text).strip().lower()\n",
        "\n",
        "tokenized_corpus = [normalize(p['text']).split() for p in passages_min]\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n",
        "corpus_ids = [p['id'] for p in passages_min]\n",
        "corpus_texts = [p['text'] for p in passages_min]\n",
        "pid2text = {p['id']: p['text'] for p in passages_min}\n",
        "\n",
        "def bm25_retrieve(query, k=50):\n",
        "    query_tokens = normalize(query).split()\n",
        "    scores = bm25.get_scores(query_tokens)\n",
        "    top_n = np.argsort(scores)[::-1][:k]\n",
        "    return [(corpus_ids[i], corpus_texts[i], float(scores[i])) for i in top_n]\n",
        "\n",
        "# 4. Build Dense Index (Base Model - Fast)\n",
        "print(\"Building Dense Index...\")\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "\n",
        "# Use CPU for embedding to save GPU memory for Qwen\n",
        "embedder = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\", device=\"cpu\")\n",
        "passage_embeddings = embedder.encode(corpus_texts, convert_to_numpy=True, show_progress_bar=True)\n",
        "\n",
        "d = passage_embeddings.shape[1]\n",
        "index = faiss.IndexFlatIP(d)\n",
        "faiss.normalize_L2(passage_embeddings)\n",
        "index.add(passage_embeddings)\n",
        "\n",
        "def dense_retrieve(query, k=50):\n",
        "    q_emb = embedder.encode([query], convert_to_numpy=True)\n",
        "    faiss.normalize_L2(q_emb)\n",
        "    D, I = index.search(q_emb, k)\n",
        "    return [(corpus_ids[i], corpus_texts[i], float(D[0][i])) for i in range(len(I[0]))]\n",
        "\n",
        "# 5. Define Hybrid Retrieve Function\n",
        "def retrieve(query, k=5, mode=\"hybrid_score\"):\n",
        "    pool_k = 50\n",
        "    bm25_hits = bm25_retrieve(query, k=pool_k)\n",
        "    dense_hits = dense_retrieve(query, k=pool_k)\n",
        "\n",
        "    def norm(hits):\n",
        "        scores = [s for _,_,s in hits]\n",
        "        if not scores: return {}\n",
        "        min_s, max_s = min(scores), max(scores)\n",
        "        if max_s == min_s: return {pid: 1.0 for pid,_,_ in hits}\n",
        "        return {pid: (s - min_s)/(max_s - min_s) for pid,_,s in hits}\n",
        "\n",
        "    bm25_scores = norm(bm25_hits)\n",
        "    dense_scores = norm(dense_hits)\n",
        "\n",
        "    all_pids = set(bm25_scores.keys()) | set(dense_scores.keys())\n",
        "    final_scores = []\n",
        "\n",
        "    for pid in all_pids:\n",
        "        s_bm25 = bm25_scores.get(pid, 0.0)\n",
        "        s_dense = dense_scores.get(pid, 0.0)\n",
        "        final_score = 0.5 * s_bm25 + 0.5 * s_dense\n",
        "        final_scores.append((pid, pid2text[pid], final_score))\n",
        "\n",
        "    final_scores.sort(key=lambda x: x[2], reverse=True)\n",
        "    return final_scores[:k]\n",
        "\n",
        "print(\"✅ COMPLETE: Data & Retriever Restored!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "0448f450ae2640dba12231d7e2ef6349",
            "01751f9b1e9645019f06d9482666de44",
            "faeb94bb9c9148bca90c3780e050a789",
            "ca0df6aabe894d20b924a08358a69663",
            "445f709782974f6ab0733e999a19fb5e",
            "dfdb322c0983427a8e7bcb31e5a67ca8",
            "1d6004dc98d8459dbb5cbed872dfff0f",
            "00286e5c493d4443850215f76be4f32e",
            "8deeefab8c5c48c682fb3326b07c41be",
            "b12d4c4373ba4c1daa5ac1cc7091252d",
            "e75569452fca44ec897af26417171d17"
          ]
        },
        "id": "pTHH9NSkXIEH",
        "outputId": "02febf83-c9be-4a5f-c55f-440ee125683a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⏳ Loading Qwen/Qwen2.5-7B-Instruct...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0448f450ae2640dba12231d7e2ef6349",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Qwen Loaded Successfully (Inference Mode)!\n"
          ]
        }
      ],
      "source": [
        "# Cell 10_Inference_Only: Load Qwen for RAG (No Training)\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "# 1. Optimizations\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# 2. Config for 4-bit Loading (Memory Safe)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# 3. Load Model\n",
        "model_id = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "\n",
        "print(f\"⏳ Loading {model_id}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(\"✅ Qwen Loaded Successfully (Inference Mode)!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iirFQO4LfFap",
        "outputId": "5359d3e0-5778-42c1-b92b-176f75bdf716"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧪 Testing RAG Pipeline...\n",
            "❓ Question: کورونا وائرس کی علامات کیا ہیں؟\n",
            "💡 Answer: کورنا وائرس کی علامات میں بخار، کھانسی اور سانس لینے میں دشواری شامل ہیں۔\n",
            "⏱️ Time: 4.05s\n"
          ]
        }
      ],
      "source": [
        "# Cell 11: RAG Inference Pipeline (Qwen Fine-Tuned + Hybrid Retriever)\n",
        "import time\n",
        "import torch\n",
        "\n",
        "# ---------- Preconditions ----------\n",
        "assert 'model' in globals(), \"Qwen Model (Cell 10) is not loaded!\"\n",
        "assert 'tokenizer' in globals(), \"Tokenizer (Cell 10) is not loaded!\"\n",
        "assert 'retrieve' in globals(), \"Retrieve function (Cell 9) is not loaded!\"\n",
        "\n",
        "# ---------- Configuration ----------\n",
        "RAG_K = 5                  # Number of documents to retrieve\n",
        "RAG_MODE = \"hybrid_score\"  # Your best retrieval mode (from Cell 12 analysis)\n",
        "\n",
        "def rag_pipeline(query, k=RAG_K, mode=RAG_MODE, debug=False):\n",
        "    \"\"\"\n",
        "    1. Retrieve Docs (Hybrid)\n",
        "    2. Format Prompt (Qwen Chat Template)\n",
        "    3. Generate Answer\n",
        "    \"\"\"\n",
        "    t0 = time.time()\n",
        "\n",
        "    # --- Step 1: Retrieval ---\n",
        "    # Get top K documents\n",
        "    retrieved_hits = retrieve(query, k=k, mode=mode)\n",
        "\n",
        "    # Deduplicate (Keep unique text)\n",
        "    seen_ids = set()\n",
        "    unique_passages = []\n",
        "    context_text = \"\"\n",
        "\n",
        "    for idx, (pid, text, score) in enumerate(retrieved_hits):\n",
        "        if pid not in seen_ids:\n",
        "            unique_passages.append((pid, text, score))\n",
        "            seen_ids.add(pid)\n",
        "            # Add to context string\n",
        "            context_text += f\"[حوالہ {len(unique_passages)}] {text}\\n\"\n",
        "\n",
        "    # --- Step 2: Prompt Engineering (The \"Doctor\" Persona) ---\n",
        "    # This matches the format we used in Fine-Tuning (Cell 10c)\n",
        "    system_prompt = \"آپ ایک ماہر ڈاکٹر ہیں۔ نیچے دی گئی 'معلومات' کی بنیاد پر صارف کے سوال کا اردو میں درست اور مختصر جواب دیں۔\"\n",
        "\n",
        "    user_prompt = f\"\"\"\n",
        "    معلومات:\n",
        "    {context_text}\n",
        "\n",
        "    سوال: {query}\n",
        "    \"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt}\n",
        "    ]\n",
        "\n",
        "    # Convert to Qwen Input IDs\n",
        "    text_input = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    model_inputs = tokenizer([text_input], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # --- Step 3: Generation ---\n",
        "    generator_success = False\n",
        "    answer = \"\"\n",
        "\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "            generated_ids = model.generate(\n",
        "                **model_inputs,\n",
        "                max_new_tokens=256,   # Allow enough space for Urdu answer\n",
        "                temperature=0.3,      # Low temp = more factual\n",
        "                top_p=0.9,\n",
        "                do_sample=True,\n",
        "                repetition_penalty=1.1 # Prevent getting stuck in loops\n",
        "            )\n",
        "\n",
        "        # Decode Output\n",
        "        generated_ids = [\n",
        "            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "        ]\n",
        "        answer = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
        "        generator_success = True\n",
        "\n",
        "    except Exception as e:\n",
        "        answer = \"Error during generation.\"\n",
        "        print(f\"❌ Gen Error: {e}\")\n",
        "\n",
        "    latency = time.time() - t0\n",
        "\n",
        "    # --- Step 4: Refusal Check ---\n",
        "    # Did the model say \"I don't know\"?\n",
        "    is_refusal = any(phrase in answer for phrase in [\"معاف کیجئے\", \"دستیاب نہیں\", \"معلومات نہیں\"])\n",
        "\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"answer\": answer,\n",
        "        \"retrieved_ids\": [p[0] for p in unique_passages],\n",
        "        \"latency\": latency,\n",
        "        \"is_refusal\": is_refusal,\n",
        "        \"success\": generator_success,\n",
        "        \"context_preview\": context_text[:200]\n",
        "    }\n",
        "\n",
        "# ---------- Smoke Test ----------\n",
        "print(\"🧪 Testing RAG Pipeline...\")\n",
        "test_q = \"کورونا وائرس کی علامات کیا ہیں؟\"\n",
        "res = rag_pipeline(test_q, debug=True)\n",
        "print(f\"❓ Question: {test_q}\")\n",
        "print(f\"💡 Answer: {res['answer']}\")\n",
        "print(f\"⏱️ Time: {res['latency']:.2f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292,
          "referenced_widgets": [
            "46b143ff58fb425891662377bbc4caf2",
            "148ff59b05a1496c8db628bc8365fc69",
            "f1b53d0850bd4c6cbdb30edd133a4d64",
            "41d2e3a24db2413287611757425417d0",
            "36bb1e446861420ba7e5bfb0d06c5bcf",
            "0f9328ba25f249e4a805ce97974e88a3",
            "24e2e7a1c57b4e0cb972efa8c992c86d",
            "833ee52debb94b69bc65c3c9c72a941f",
            "647b8663b5a1431ab2154ef20bf28de8",
            "b8a6a9dd31f341dfaed56eab637f0886",
            "bdcb802c3652424f9e84e1102c52bd4f"
          ]
        },
        "id": "nNyWTN33fJTa",
        "outputId": "2798d5fa-0386-4e0b-ca99-aecbf0f175a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting Final Evaluation on 100 queries...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "46b143ff58fb425891662377bbc4caf2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "🏆 FINAL PROJECT REPORT: URDU COVID-19 RAG\n",
            "==================================================\n",
            "Model:     Qwen2.5-7B-Instruct (Fine-Tuned Adapter)\n",
            "Retriever: Hybrid (Dense + BM25)\n",
            "------------------------------\n",
            "✅ BLEU Score:   85.55   (Previous Best: ~15.8)\n",
            "✅ chrF Score:   96.42   (Previous Best: ~31.2)\n",
            "⏱️ Avg Latency:  10.875s\n",
            "🚫 Refusals:     0/100 (0.0%)\n",
            "==================================================\n",
            "📄 Detailed logs saved to: /content/drive/MyDrive/eval_outputs/final_qwen_rag_results.json\n"
          ]
        }
      ],
      "source": [
        "# Cell 12: Comprehensive RAG Evaluation\n",
        "import statistics\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Install sacrebleu if missing\n",
        "try:\n",
        "    import sacrebleu\n",
        "except ImportError:\n",
        "    !pip install -q sacrebleu\n",
        "    import sacrebleu\n",
        "\n",
        "# Ensure we have data\n",
        "assert 'eval_queries' in globals(), \"eval_queries not loaded!\"\n",
        "\n",
        "print(f\"🚀 Starting Final Evaluation on {len(eval_queries)} queries...\")\n",
        "\n",
        "predictions = []\n",
        "references = []\n",
        "latencies = []\n",
        "refusal_count = 0\n",
        "per_query_results = []\n",
        "\n",
        "# Loop through all evaluation queries\n",
        "for item in tqdm(eval_queries):\n",
        "    query = item['query']\n",
        "    gold_answer = item['gold_answer']\n",
        "\n",
        "    # Run the Pipeline\n",
        "    # Note: We use the best mode 'hybrid_score' automatically\n",
        "    result = rag_pipeline(query)\n",
        "\n",
        "    # Store Data\n",
        "    predictions.append(result['answer'])\n",
        "    references.append([gold_answer]) # sacrebleu needs list of lists\n",
        "    latencies.append(result['latency'])\n",
        "\n",
        "    if result['is_refusal']:\n",
        "        refusal_count += 1\n",
        "\n",
        "    # Log per query for inspection\n",
        "    per_query_results.append({\n",
        "        \"query\": query,\n",
        "        \"gold\": gold_answer,\n",
        "        \"generated\": result['answer'],\n",
        "        \"retrieved\": result['retrieved_ids'],\n",
        "        \"latency\": result['latency']\n",
        "    })\n",
        "\n",
        "# --- Calculate Metrics ---\n",
        "bleu = sacrebleu.corpus_bleu(predictions, references)\n",
        "chrf = sacrebleu.corpus_chrf(predictions, references)\n",
        "avg_latency = statistics.mean(latencies)\n",
        "refusal_rate = (refusal_count / len(eval_queries)) * 100\n",
        "\n",
        "# --- Save Results ---\n",
        "OUT_FILE = \"/content/drive/MyDrive/eval_outputs/final_qwen_rag_results.json\"\n",
        "os.makedirs(os.path.dirname(OUT_FILE), exist_ok=True)\n",
        "with open(OUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump({\n",
        "        \"metrics\": {\n",
        "            \"BLEU\": bleu.score,\n",
        "            \"chrF\": chrf.score,\n",
        "            \"Latency\": avg_latency,\n",
        "            \"Refusal_Rate\": refusal_rate\n",
        "        },\n",
        "        \"details\": per_query_results\n",
        "    }, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# --- Print Report Card ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"🏆 FINAL PROJECT REPORT: URDU COVID-19 RAG\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Model:     Qwen2.5-7B-Instruct (Fine-Tuned Adapter)\")\n",
        "print(f\"Retriever: Hybrid (Dense + BM25)\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"✅ BLEU Score:   {bleu.score:.2f}   (Previous Best: ~15.8)\")\n",
        "print(f\"✅ chrF Score:   {chrf.score:.2f}   (Previous Best: ~31.2)\")\n",
        "print(f\"⏱️ Avg Latency:  {avg_latency:.3f}s\")\n",
        "print(f\"🚫 Refusals:     {refusal_count}/{len(eval_queries)} ({refusal_rate:.1f}%)\")\n",
        "print(\"=\"*50)\n",
        "print(f\"📄 Detailed logs saved to: {OUT_FILE}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell to keep my notebook awake\n",
        "print(\"This will keep it awake. Run after 1 minute intervals!!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-jX9UsRo0ts",
        "outputId": "1e59bd3b-224a-4e1c-986c-e43acfc376f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This will keep it awake. Run after 1 minute intervals!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9nyT-_FcpCel"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
